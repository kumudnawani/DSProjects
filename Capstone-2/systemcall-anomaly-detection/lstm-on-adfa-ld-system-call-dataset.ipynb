{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System call Anomaly Detection- Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADFA Dataset Preprocessing:**\n",
    "\n",
    "    1. The system call language model estimates the probability distribution of the next call in a sequence given the sequence of previous calls. \n",
    "       \n",
    "    2. We assume that the host system generates a finite number of system calls. \n",
    "    \n",
    "    3. We index each system call by using an integer starting from 1 and denote the fixed set of all possible system calls in the system as S = {1, · · · , K}. Let x = x1x2 · · · xl(xi ∈ S) denote a sequence of l system calls.\n",
    "       \n",
    "**LSTM Based Model :**     \n",
    "\n",
    "    1. At the Input Layer, the call at each time step xi is fed into the model in the form of one-hot encoding,\n",
    "       in other words, a K dimensional vector with all elements zero except position xi.\n",
    "       \n",
    "    2. At the Embedding Layer*, incoming calls are embedded to continuous space by multiplying embedding matrix W,\n",
    "       which should be learned. \n",
    "       \n",
    "    3. At the Hidden Layer*, the LSTM unit has an internal state, and this state is updated recurrently at each time step.\n",
    "    \n",
    "    4. At the Output Layer, a softmax activation function is used to produce the estimation of normalized probability values of possible calls coming next in the sequence.\n",
    "    \n",
    "**References for systemcalls:**\n",
    "    1. http://osinside.net/syscall/system_call_table.htm\n",
    "    2. https://www.cs.unm.edu/~immsec/systemcalls.htm    \n",
    "    3. https://github.com/karpathy/char-rnn\n",
    "    4. https://keras.io/losses/#categorical_crossentropy\n",
    "    5. http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADFA Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  1 13:52:35 2019\n",
    "\n",
    "@author: kuna\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# ignore all user warnings\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "def saveintopickle(obj, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print (\"[Pickle]: save object into {}\".format(filename))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def loadfrompickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "\n",
    "\n",
    "#draw the  process bar\n",
    "def drawProgressBar(percent, barLen = 20):\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        else:\n",
    "            progress += \" \"\n",
    "    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import io_helper\n",
    "\n",
    "\n",
    "random_data_dup = 10  # each sample randomly duplicated between 0 and 9 times, see dropin function\n",
    "\n",
    "\n",
    "def dropin(X, y):\n",
    "    \"\"\"\n",
    "    The name suggests the inverse of dropout, i.e. adding more samples. See Data Augmentation section at\n",
    "    http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/\n",
    "    :param X: Each row is a training sequence\n",
    "    :param y: Tne target we train and will later predict\n",
    "    :return: new augmented X, y\n",
    "    \"\"\"\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    X_hat = []\n",
    "    y_hat = []\n",
    "    for i in range(0, len(X)):\n",
    "        for j in range(0, np.random.random_integers(0, random_data_dup)):\n",
    "            X_hat.append(X[i, :])\n",
    "            y_hat.append(y[i])\n",
    "    return np.asarray(X_hat), np.asarray(y_hat)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    arrayfile = \"./array_test.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_train = array[:,:-1]\n",
    "    y_train = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_train.shape)\n",
    "    print (y_train.shape)\n",
    "    return (x_train,y_train)\n",
    "\n",
    "def preprocess_val():\n",
    "\n",
    "    arrayfile = \"./array_val.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_test = array[:,:-1]\n",
    "    y_test = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_test.shape)\n",
    "    print (y_test.shape)\n",
    "    return (x_test,y_test)\n",
    "\n",
    "#if __name__ ==\"__main__\":\n",
    "#   preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834\n",
      "Skip the file ADFA-LD/Training_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 63, 64, 65, 66, 75, 77, 78, 83, 85, 91, 93, 94, 96, 97, 99, 102, 104, 110, 114, 117, 118, 119, 120, 122, 125, 128, 132, 133, 140, 141, 142, 143, 144, 146, 148, 155, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 179, 180, 183, 184, 185, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 219, 220, 221, 224, 226, 228, 229, 230, 231, 233, 234, 240, 242, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 298, 300, 301, 307, 308, 309, 311, 314, 320, 322, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "833\n",
      "The maximum length of a sequence is that 2948\n",
      "lists_of_list_into_big_matrix\n",
      "833\n",
      "[ =                    ] 8.52%(20298, 20, 341)\n",
      "done\n",
      "[Pickle]: save object into array_test.pickle\n",
      "4373\n",
      "Skip the file ADFA-LD/Validation_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 22, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 61, 63, 64, 65, 66, 75, 77, 78, 79, 83, 85, 90, 91, 93, 94, 96, 97, 99, 102, 104, 110, 111, 114, 116, 117, 118, 119, 120, 122, 124, 125, 128, 132, 133, 136, 140, 141, 142, 143, 144, 146, 148, 150, 151, 154, 155, 156, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 190, 191, 192, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 224, 226, 228, 229, 231, 234, 240, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 296, 298, 300, 301, 306, 307, 308, 309, 311, 314, 320, 324, 328, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "4372\n",
      "The maximum length of a sequence is that 4494\n",
      "[                      ] 1.26%(21238, 20, 341)\n",
      "done\n",
      "[Pickle]: save object into array_val.pickle\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "#import io_helper\n",
    "\n",
    "def readfilesfromAdir(dataset):\n",
    "    #read a list of files\n",
    "    files = os.listdir(dataset)\n",
    "    files_absolute_paths = []\n",
    "    for i in files:\n",
    "        files_absolute_paths.append(dataset+str(i))\n",
    "    return files_absolute_paths\n",
    "\n",
    "\n",
    "file = \"ADFA-LD/Training_Data_Master/UTD-0001.txt\"\n",
    "#this is used to read a char sequence from\n",
    "def readCharsFromFile(file):\n",
    "    channel_values = open(file).read().split()\n",
    "    #print (len(channel_values))\n",
    "    #channel_values is a list\n",
    "    return channel_values\n",
    "    #print (channel_values[800:819])\n",
    "\n",
    "def get_attack_subdir(path):\n",
    "    subdirectories = os.listdir(path)\n",
    "    for i in range(0,len(subdirectories)):\n",
    "        subdirectories[i] = path + subdirectories[i]\n",
    "\n",
    "    print (subdirectories)\n",
    "    return (subdirectories)\n",
    "\n",
    "\n",
    "def get_all_call_sequences(dire):\n",
    "    files = readfilesfromAdir(dire)\n",
    "    allthelist = []\n",
    "    print (len(files))\n",
    "\n",
    "    for eachfile in files:\n",
    "        if not eachfile.endswith(\"DS_Store\"):\n",
    "            allthelist.append(readCharsFromFile(eachfile))\n",
    "        else:\n",
    "            print (\"Skip the file \"+ str(eachfile))\n",
    "\n",
    "    elements = []\n",
    "    for item in allthelist:\n",
    "        for key in item:\n",
    "            if key not in elements:\n",
    "                elements.append(key)\n",
    "\n",
    "    elements = map(int,elements)\n",
    "    elements = sorted(elements)\n",
    "\n",
    "    print (\"The total unique elements:\")\n",
    "    print (elements)\n",
    "\n",
    "    print (\"The maximum number of elements:\")\n",
    "    print (max(elements))\n",
    "\n",
    "    #print (\"The length elements:\")\n",
    "    #print (len(elements))\n",
    "    print (len(allthelist))\n",
    "\n",
    "    #clean the all list data set\n",
    "    _max = 0\n",
    "    for i in range(0,len(allthelist)):\n",
    "        _max = max(_max,len(allthelist[i]))\n",
    "        allthelist[i] = list(map(int,allthelist[i]))\n",
    "        #print(allthelist[i])\n",
    "\n",
    "\n",
    "    print (\"The maximum length of a sequence is that {}\".format(_max))\n",
    "\n",
    "    return (allthelist)\n",
    "\n",
    "## shift the data for analysis\n",
    "def shift(seq, n):\n",
    "    n = n % len(seq)\n",
    "    return seq[n:] + seq[:n]\n",
    "\n",
    "\n",
    "def convertToOneHot(vector, num_classes=None):\n",
    "    \"\"\"\n",
    "    Converts an input 1-D vector of integers into an output\n",
    "    2-D array of one-hot vectors, where an i'th input value\n",
    "    of j will set a '1' in the i'th row, j'th column of the\n",
    "    output array.\n",
    "\n",
    "    Example:\n",
    "        v = np.array((1, 0, 4))\n",
    "        one_hot_v = convertToOneHot(v)\n",
    "        print one_hot_v\n",
    "\n",
    "        [[0 1 0 0 0]\n",
    "         [1 0 0 0 0]\n",
    "         [0 0 0 0 1]]\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(vector, np.ndarray)\n",
    "    assert len(vector) > 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(vector)+1\n",
    "    else:\n",
    "        assert num_classes > 0\n",
    "        assert num_classes >= np.max(vector)\n",
    "\n",
    "    result = np.zeros(shape=(len(vector), num_classes))\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result.astype(int)\n",
    "\n",
    "\"\"\"\n",
    "The num_class here is set as 341\n",
    "\"\"\"\n",
    "\n",
    "#one function do one thing\n",
    "def sequence_n_gram_parsing(alist,n_gram=20,num_class=341):\n",
    "    if len(alist) <= n_gram:\n",
    "        return alist\n",
    "\n",
    "    ans = []\n",
    "    for i in range(0,len(alist)-n_gram+1,1):\n",
    "        tmp = alist[i:i+n_gram]\n",
    "        oneHot = convertToOneHot(np.asarray(tmp), num_class)\n",
    "        #print(tmp)\n",
    "        #print(np.asarray(tmp))\n",
    "        #print(oneHot)\n",
    "        ans.append(oneHot)\n",
    "\n",
    "    #transform into nmup arrray\n",
    "    ans = np.array(ans)\n",
    "    return (ans)\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix(allthelist,n_gram=20):\n",
    "    \n",
    "    print(\"lists_of_list_into_big_matrix\")\n",
    "    print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "    #print(len(allthelist[0]))\n",
    "    #print(allthelist[0])\n",
    "    #print(len(array))\n",
    "    #print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "       \n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "       \n",
    "        #print (\"tmp shape\")\n",
    "        #print(tmp)\n",
    "        #print (len(tmp))\n",
    " \n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "        #print(allthelist[i])\n",
    "        #print(array)\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "        #print(len(allthelist[1]))\n",
    "        #print(allthelist[1])\n",
    "        #print(len(array))\n",
    "        #print(array)\n",
    "        #break\n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_test.pickle\")\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix_val(allthelist,n_gram=20):\n",
    "\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_val.pickle\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dirc = \"ADFA-LD/Training_Data_Master/\"\n",
    "    dirc_val = \"ADFA-LD/Validation_Data_Master/\"\n",
    "    dic_attack =\"ADFA-LD/Attack_Data_Master/Adduser_1/\"\n",
    "    #train1 = get_all_call_sequences(dirc)\n",
    "\n",
    "    #test = [i for i in range(0,300)]\n",
    "    #array = sequence_n_gram_parsing(test)\n",
    "    #print (type(array))\n",
    "    #print (array.shape)\n",
    "\n",
    "    #get_attack_subdir(dic_attack)\n",
    "    #print (\"XxxxxxxXXXXXXXXXXX\")\n",
    "    #val1 = get_all_call_sequences(dirc_val)\n",
    "    \n",
    "    #dirc_test = \"Test/\"\n",
    "    #att_test = get_all_call_sequences(dirc_test)\n",
    "    #lists_of_list_into_big_matrix(att_test)\n",
    "    \n",
    "    att = get_all_call_sequences(dirc)\n",
    "    lists_of_list_into_big_matrix(att)\n",
    "    \n",
    "    att_val = get_all_call_sequences(dirc_val)\n",
    "    lists_of_list_into_big_matrix_val(att_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "#import preprocess\n",
    "\n",
    "# Global hyper-parameters\n",
    "sequence_length = 19\n",
    "epochs = 1\n",
    "batch_size = 50\n",
    "feature_dimension = 341\n",
    "top_words = 5000\n",
    "\n",
    "def save_model_weight_into_file(model, modelname=\"model.json\", weight=\"model.h5\"):\n",
    "    model_json = model.to_json()\n",
    "    with open(modelname, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(weight)\n",
    "    print(\"Saved model to disk in {} and {}\".format(modelname,weight))\n",
    "\n",
    "\n",
    "def load_model_and_wieght_from_file(modelname=\"model.json\", weight=\"model.h5\"):\n",
    "\n",
    "    json_file = open(modelname, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weight)\n",
    "    print(\"Loaded model from disk, you can do more analysis more\")\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    layers = {'input': feature_dimension, 'hidden1': 64, 'hidden2': 256, 'hidden3': 100, 'output': feature_dimension}\n",
    "\n",
    "    model.add(LSTM(\n",
    "            input_length=sequence_length,\n",
    "            input_dim=layers['input'],\n",
    "            output_dim=layers['hidden1'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden2'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden3'],\n",
    "            return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "            output_dim=layers['output'],activation='softmax'))\n",
    "    #model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop',  metrics=['accuracy'])\n",
    "    #model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def run_network(model=None, data=None):\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    \n",
    "    if data is None:\n",
    "        print ('Loading data... ')\n",
    "        # train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "        X_train, y_train  = preprocess()\n",
    "    else:\n",
    "        X_train, y_train = data\n",
    "\n",
    "    print (\"X_train, y_train,shape\")\n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "    if model is None:\n",
    "        model = build_model()\n",
    "        #model = build_model_2()\n",
    "        print(\"Training...\")\n",
    "        model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_split=0.3)\n",
    "        model.summary()\n",
    "        print(\"Done Training...\")\n",
    "\n",
    "    #predicted = model.predict(X_test)\n",
    "    #print(\"Reshaping predicted\")\n",
    "    #predicted = np.reshape(predicted, (predicted.size,))\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"prediction exception\")\n",
    "        print 'Training duration (s) : ', time.time() - global_start_time\n",
    "        return model, y_test, 0\n",
    "   \n",
    "    try:\n",
    "        plt.figure(1)\n",
    "        plt.subplot(311)\n",
    "        plt.title(\"Actual Test Signal w/Anomalies\")\n",
    "        plt.plot(y_test[:len(y_test)], 'b')\n",
    "        plt.subplot(312)\n",
    "        plt.title(\"Predicted Signal\")\n",
    "        plt.plot(predicted[:len(y_test)], 'g')\n",
    "        plt.subplot(313)\n",
    "        plt.title(\"Squared Error\")\n",
    "        mse = ((y_test - predicted) ** 2)\n",
    "        plt.plot(mse, 'r')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"plotting exception\")\n",
    "        print (str(e))\n",
    "    print ('Training duration (s) : '% (time.time() - global_start_time))\n",
    "\n",
    "    return model, y_test, predicted\n",
    "   \"\"\"\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "# run_network()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kuna\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\kuna\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Training...\n",
      "WARNING:tensorflow:From C:\\Users\\kuna\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 14208 samples, validate on 6090 samples\n",
      "Epoch 1/1\n",
      "14208/14208 [==============================] - 32s 2ms/step - loss: 2.7770 - acc: 0.2367 - val_loss: 2.8779 - val_acc: 0.2154\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 19, 64)            103936    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 19, 256)           328704    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 19, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               142800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 341)               34441     \n",
      "=================================================================\n",
      "Total params: 609,881\n",
      "Trainable params: 609,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "if model is None:\n",
    "    model = build_model()\n",
    "    print(\"Training...\")\n",
    "    history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.3,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    model.summary()\n",
    "    print(\"Done Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "#def loadData(file): \n",
    "    # for reading also binary mode is important \n",
    "#    dbfile = open(file, 'rb')      \n",
    "#    db = pickle.load(dbfile) \n",
    "#    for keys in db: \n",
    "#        print(keys, '=>', db[keys]) \n",
    "#    dbfile.close() \n",
    "  \n",
    "#if __name__ == '__main__': \n",
    "#    loadData(\"./array_test.pickle\") \n",
    "#df_val = pd.read_pickle(\"./array_val.pickle\")\n",
    "#df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data size is that \n",
      "(21238, 19, 341)\n",
      "(21238, 341)\n",
      "X_test, y_test,shape\n",
      "(21238, 19, 341)\n",
      "(21238, 341)\n",
      "Validating...\n",
      "Done Validating...\n",
      "[[2.8147128e-05 3.8867351e-02 5.9301241e-05 ... 3.2527638e-05\n",
      "  4.0639268e-05 6.0572522e-04]\n",
      " [2.7337572e-05 4.2425249e-02 5.7118334e-05 ... 3.0709063e-05\n",
      "  3.9311104e-05 5.8961258e-04]\n",
      " [3.2080068e-05 4.1573644e-02 6.1957377e-05 ... 3.6363443e-05\n",
      "  4.3218708e-05 6.0780835e-04]\n",
      " ...\n",
      " [1.8595829e-06 1.2511486e-03 3.5294606e-06 ... 2.1812916e-06\n",
      "  1.6237399e-06 6.7461682e-05]\n",
      " [1.8240867e-06 1.3079355e-03 3.5116327e-06 ... 2.1674750e-06\n",
      "  1.6114174e-06 6.7553679e-05]\n",
      " [1.8013474e-06 1.3025296e-03 3.4824586e-06 ... 2.1350809e-06\n",
      "  1.6007832e-06 6.6800356e-05]]\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "X_test, y_test = preprocess_val()\n",
    "\n",
    "print (\"X_test, y_test,shape\")\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "print(\"Validating...\")\n",
    "predicted = model.predict(X_test)\n",
    "print(\"Done Validating...\")\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did our model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 3.08\n",
      "Validation Accuracy : 0.19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Score : %.2f'%(score))\n",
    "print('Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Loss')\n",
    "#plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [2.8778519998434535],\n",
       " 'val_acc': [0.21543513882556573],\n",
       " 'loss': [2.776978516363883],\n",
       " 'acc': [0.23669763501452468]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Accuracy')\n",
    "#plt.plot(history.history['acc'], label='train')\n",
    "#plt.plot(history.history['val_acc'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Test with new systemcall  sequence ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hidden nodes is 100.\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "\n",
    "word_vec_length = 19\n",
    "char_vec_length = 341\n",
    "output_labels = 341\n",
    "\n",
    "\n",
    "#hidden_nodes = 4000 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "hidden_nodes = 100\n",
    "print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "def build_model_2():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 2.8397 - acc: 0.2396 - val_loss: 2.5586 - val_acc: 0.4236\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.9795 - acc: 0.4184 - val_loss: 2.1248 - val_acc: 0.4901\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.6778 - acc: 0.5024 - val_loss: 2.0376 - val_acc: 0.5023\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.5040 - acc: 0.5570 - val_loss: 1.9283 - val_acc: 0.5027\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 20s 977us/step - loss: 1.3988 - acc: 0.5863 - val_loss: 1.9624 - val_acc: 0.5107\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 20s 1ms/step - loss: 1.3225 - acc: 0.6038 - val_loss: 1.9617 - val_acc: 0.5203\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.2664 - acc: 0.6213 - val_loss: 1.9898 - val_acc: 0.5073\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 20s 998us/step - loss: 1.2196 - acc: 0.6373 - val_loss: 2.0098 - val_acc: 0.5102\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.1759 - acc: 0.6471 - val_loss: 2.0388 - val_acc: 0.5134\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 20s 980us/step - loss: 1.1451 - acc: 0.6549 - val_loss: 2.0311 - val_acc: 0.5157\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_2()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score : 1.05\n",
      "Train Validation Accuracy : 0.68\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_train, y_train, verbose=2, batch_size=batch_size)\n",
    "print('Train Score : %.2f'%(score))\n",
    "print('Train Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score : 2.03\n",
      "Test Validation Accuracy : 0.52\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Test Score : %.2f'%(score))\n",
    "print('Test Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## k-fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train\n",
    "Y = y_train\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 10-fold cross validation test harness\n",
    "#kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#cvscores = []\n",
    "#for train, test in kfold.split(X, Y):\n",
    "#  # create model\n",
    "#\tmodel = Sequential()\n",
    "#\tmodel.add(Dense(12, input_dim=341, activation='relu'))\n",
    "#\tmodel.add(Dense(8, activation='relu'))\n",
    "#\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "#\t# Compile model\n",
    "#\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\t# Fit the model\n",
    "#\tmodel.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "#\t# evaluate the model\n",
    "#\tscores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "#\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#\tcvscores.append(scores[1] * 100)\n",
    "#print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hidden nodes is 100.\n",
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 24s 1ms/step - loss: 0.0118 - acc: 0.9971 - val_loss: 0.0100 - val_acc: 0.9973\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 20s 1ms/step - loss: 0.0086 - acc: 0.9973 - val_loss: 0.0086 - val_acc: 0.9976 0.99 - ETA: 5s - loss: 0. - ETA: 4s - loss: 0.00 - ETA: 3s - loss: 0.0087 - acc: 0.9 - ETA: 3s - loss: 0.0087 - - ETA: 2s - loss: 0. - ETA: 1s - loss: 0.\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 23s 1ms/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0083 - val_acc: 0.9977\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 24s 1ms/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0078 - val_acc: 0.9977\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 20s 1ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0077 - val_acc: 0.9978\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 20s 994us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "\n",
    "word_vec_length = 19\n",
    "char_vec_length = 341\n",
    "output_labels = 341\n",
    "\n",
    "\n",
    "hidden_nodes = 100 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "def build_model_3():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_3()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 23s 1ms/step - loss: 0.0112 - acc: 0.9971 - val_loss: 0.0099 - val_acc: 0.9975\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 20s 986us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0085 - val_acc: 0.9977\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 20s 996us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0078 - val_acc: 0.9977\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0079 - val_acc: 0.9977\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 20s 994us/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "def build_model_4():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_4()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 28s 1ms/step - loss: 0.0111 - acc: 0.9971 - val_loss: 0.0100 - val_acc: 0.9973\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.0085 - val_acc: 0.9977\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0081 - val_acc: 0.9978\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0079 - val_acc: 0.9978\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0081 - val_acc: 0.9977\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0081 - val_acc: 0.9977\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0082 - val_acc: 0.9978\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "def build_model_5():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_5()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score : 0.00\n",
      "Train Validation Accuracy : 1.00\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_train, y_train, verbose=2, batch_size=batch_size)\n",
    "print('Train Score : %.2f'%(score))\n",
    "print('Train Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score : 0.01\n",
      "Test Validation Accuracy : 1.00\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Test Score : %.2f'%(score))\n",
    "print('Test Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for Binary Classification of SystemCalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834\n",
      "Skip the file ADFA-LD/Training_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 63, 64, 65, 66, 75, 77, 78, 83, 85, 91, 93, 94, 96, 97, 99, 102, 104, 110, 114, 117, 118, 119, 120, 122, 125, 128, 132, 133, 140, 141, 142, 143, 144, 146, 148, 155, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 179, 180, 183, 184, 185, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 219, 220, 221, 224, 226, 228, 229, 230, 231, 233, 234, 240, 242, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 298, 300, 301, 307, 308, 309, 311, 314, 320, 322, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "833\n",
      "The maximum length of a sequence is that 2948\n",
      "[ ==                   ] 14.53%(40099, 20)\n",
      "done\n",
      "[Pickle]: save object into array_test.pickle\n",
      "4373\n",
      "Skip the file ADFA-LD/Validation_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 22, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 61, 63, 64, 65, 66, 75, 77, 78, 79, 83, 85, 90, 91, 93, 94, 96, 97, 99, 102, 104, 110, 111, 114, 116, 117, 118, 119, 120, 122, 124, 125, 128, 132, 133, 136, 140, 141, 142, 143, 144, 146, 148, 150, 151, 154, 155, 156, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 190, 191, 192, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 224, 226, 228, 229, 231, 234, 240, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 296, 298, 300, 301, 306, 307, 308, 309, 311, 314, 320, 324, 328, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "4372\n",
      "The maximum length of a sequence is that 4494\n",
      "[                      ] 2.13%(40142, 20)\n",
      "done\n",
      "[Pickle]: save object into array_val.pickle\n",
      "The total unique elements:\n",
      "[3, 4, 5, 6, 7, 13, 19, 33, 43, 45, 54, 60, 78, 91, 102, 104, 119, 120, 122, 140, 142, 146, 162, 168, 175, 183, 192, 195, 196, 197, 220, 221, 240, 265, 268, 292, 331, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "16\n",
      "The maximum length of a sequence is that 2161\n",
      "[ ==================   ] 93.75%(6184, 20)\n",
      "done\n",
      "[Pickle]: save object into array_attack.pickle\n",
      "(40099, 20)\n",
      "The train data size is that \n",
      "(40099, 20, 1)\n",
      "(40099, 1)\n",
      "(40142, 20)\n",
      "The validation data size is that \n",
      "(40142, 20, 1)\n",
      "(40142, 1)\n",
      "The attack data size is that \n",
      "(6184, 20, 1)\n",
      "(6184, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess():\n",
    "\n",
    "    arrayfile = \"./array_test.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_train = array[:,:]\n",
    "    print (x_train.shape)\n",
    "    x_train = x_train.reshape(40099, 20, 1)\n",
    "    y_train = np.zeros((40099,1))\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_train.shape)\n",
    "    print (y_train.shape)\n",
    "    return (x_train,y_train)\n",
    "\n",
    "def preprocess_val():\n",
    "\n",
    "    arrayfile = \"./array_val.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_test = array[:,:]\n",
    "    print (x_test.shape)\n",
    "    x_test = x_test.reshape(40142, 20, 1)\n",
    "    y_test = np.zeros((40142,1))\n",
    "\n",
    "    print (\"The validation data size is that \")\n",
    "    print (x_test.shape)\n",
    "    print (y_test.shape)\n",
    "    return (x_test,y_test)\n",
    "\n",
    "def preprocess_attack():\n",
    "\n",
    "    arrayfile = \"./array_attack.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_attack = array[:,:]\n",
    "    x_attack = x_attack.reshape(6184, 20, 1)\n",
    "    y_attack = np.ones((6184,1))\n",
    "\n",
    "    print (\"The attack data size is that \")\n",
    "    print (x_attack.shape)\n",
    "    print (y_attack.shape)\n",
    "    return (x_attack,y_attack)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The num_class here is set as 1\n",
    "\"\"\"\n",
    "\n",
    "#one function do one thing\n",
    "def sequence_n_gram_parsing_noencoding(alist,n_gram=20,num_class=1):\n",
    "    if len(alist) <= n_gram:\n",
    "        return alist\n",
    "\n",
    "    ans = []\n",
    "    for i in range(0,len(alist)-n_gram+1,1):\n",
    "        tmp = alist[i:i+n_gram]\n",
    "        #oneHot = convertToOneHot(np.asarray(tmp), num_class)\n",
    "        #print(tmp)\n",
    "        #print(np.asarray(tmp))\n",
    "        #print(oneHot)\n",
    "        ans.append(tmp)\n",
    "\n",
    "    #transform into nmup arrray\n",
    "    ans = np.array(ans)\n",
    "    return (ans)\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix(allthelist,n_gram=20):\n",
    "    \n",
    "    #print(\"lists_of_list_into_big_matrix train\")\n",
    "    #print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing_noencoding(allthelist[0])\n",
    "    #print(len(allthelist[0]))\n",
    "    #print(allthelist[0])\n",
    "    #print(len(array))\n",
    "    #print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "       \n",
    "        tmp = sequence_n_gram_parsing_noencoding(allthelist[i])\n",
    "       \n",
    "        #print (\"tmp shape\")\n",
    "        #print(tmp)\n",
    "        #print (len(tmp))\n",
    " \n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "        #print(allthelist[i])\n",
    "        #print(array)\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 40000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "        #print(len(allthelist[1]))\n",
    "        #print(allthelist[1])\n",
    "        #print(len(array))\n",
    "        #print(array)\n",
    "        #break\n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_test.pickle\")\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix_val(allthelist,n_gram=20):\n",
    "\n",
    "    #print(\"lists_of_list_into_big_matrix validation\")\n",
    "    #print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing_noencoding(allthelist[0])\n",
    "    #print(len(allthelist[0]))\n",
    "    #print(allthelist[0])\n",
    "    #print(len(array))\n",
    "    #print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing_noencoding(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 40000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_val.pickle\")\n",
    "\n",
    "def get_all_call_sequences_attack(dire):\n",
    "    # list of attacks\n",
    "    attack = ['Adduser','Hydra_FTP','Hydra_SSH','Java_Meterpreter','Meterpreter','Web_Shell']\n",
    "    #attack = ['Adduser' ,'Hydra_FTP']\n",
    "    for term in attack:\n",
    "        in_address = dire+term          \n",
    "        for i in range (1,11):\n",
    "            files = readfilesfromAdir(in_address+\"_\"+str(i)+\"/\")\n",
    "        \n",
    "    allthelist = []\n",
    "    #print(files)\n",
    "    #print (len(files))\n",
    "\n",
    "    for eachfile in files:\n",
    "        if not eachfile.endswith(\"DS_Store\"):\n",
    "            allthelist.append(readCharsFromFile(eachfile))\n",
    "        else:\n",
    "            print (\"Skip the file \"+ str(eachfile))\n",
    "\n",
    "    elements = []\n",
    "    for item in allthelist:\n",
    "        for key in item:\n",
    "            if key not in elements:\n",
    "                elements.append(key)\n",
    "\n",
    "    elements = map(int,elements)\n",
    "    elements = sorted(elements)\n",
    "\n",
    "    print (\"The total unique elements:\")\n",
    "    print (elements)\n",
    "\n",
    "    print (\"The maximum number of elements:\")\n",
    "    print (max(elements))\n",
    "\n",
    "    #print (\"The length elements:\")\n",
    "    #print (len(elements))\n",
    "    print (len(allthelist))\n",
    "\n",
    "    #clean the all list data set\n",
    "    _max = 0\n",
    "    for i in range(0,len(allthelist)):\n",
    "        _max = max(_max,len(allthelist[i]))\n",
    "        allthelist[i] = list(map(int,allthelist[i]))\n",
    "        #print(allthelist[i])\n",
    "\n",
    "\n",
    "    print (\"The maximum length of a sequence is that {}\".format(_max))\n",
    "\n",
    "    return (allthelist)\n",
    "\n",
    "def lists_of_list_into_big_matrix_attack(allthelist,n_gram=20):\n",
    "\n",
    "    array = sequence_n_gram_parsing_noencoding(allthelist[0])\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing_noencoding(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 40000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_attack.pickle\")\n",
    "    #pickle2csv(\"array_attack.pickle\", \"attack.csv\")\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    dirc = \"ADFA-LD/Training_Data_Master/\"\n",
    "    dirc_val = \"ADFA-LD/Validation_Data_Master/\"\n",
    "    dic_attack =\"ADFA-LD/Attack_Data_Master/\"\n",
    "    \n",
    "    att = get_all_call_sequences(dirc)\n",
    "    lists_of_list_into_big_matrix(att)\n",
    "    \n",
    "    att_val = get_all_call_sequences(dirc_val)\n",
    "    lists_of_list_into_big_matrix_val(att_val)\n",
    "    \n",
    "    att_attack = get_all_call_sequences_attack(dic_attack)\n",
    "    lists_of_list_into_big_matrix_attack(att_attack)\n",
    "    \n",
    "    test_split = 0.2\n",
    "    \n",
    "    X_train_p, y_train_p = preprocess()\n",
    "    \n",
    "    X_test_p, y_test_p = preprocess_val()\n",
    "\n",
    "    X_attack_p, y_attack_p = preprocess_attack()\n",
    "    \n",
    "    X_a1, X_a2 = np.array_split(X_attack_p, 2)\n",
    "    y_a1, y_a2 = np.array_split(y_attack_p, 2)\n",
    "    \n",
    "    X_train = np.concatenate([X_train_p, X_a1])\n",
    "    y_train = np.concatenate([y_train_p, y_a1])\n",
    "    \n",
    "    X_test = np.concatenate([X_test_p, X_a2])\n",
    "    y_test = np.concatenate([y_test_p, y_a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43191, 20, 1),\n",
       " (43191, 1),\n",
       " (43234, 20, 1),\n",
       " (43234, 1),\n",
       " (6184, 20, 1),\n",
       " (6184, 1))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape , X_attack.shape, y_attack.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint.pprint(X_train[:,1,0])\n",
    "#pprint.pprint(y_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "X_train, y_train,shape\n",
      "(43191, 20, 1)\n",
      "(43191, 1)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 30233 samples, validate on 12958 samples\n",
      "Epoch 1/10\n",
      "30233/30233 [==============================] - 67s 2ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.5816 - val_acc: 0.7614\n",
      "Epoch 2/10\n",
      "30233/30233 [==============================] - 65s 2ms/step - loss: 9.0514e-06 - acc: 1.0000 - val_loss: 2.8807 - val_acc: 0.7614\n",
      "Epoch 3/10\n",
      "30233/30233 [==============================] - 64s 2ms/step - loss: 3.2139e-06 - acc: 1.0000 - val_loss: 3.0868 - val_acc: 0.7614\n",
      "Epoch 4/10\n",
      "30233/30233 [==============================] - 64s 2ms/step - loss: 1.5119e-06 - acc: 1.0000 - val_loss: 3.2453 - val_acc: 0.7614\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 400)               643200    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 401       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 643,603\n",
      "Trainable params: 643,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "word_vec_length = 20\n",
    "char_vec_length = 1\n",
    "output_labels = 1\n",
    "hidden_nodes = 400 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "def build_model_6():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    #model.add(Activation('softmax'))\n",
    "    model.add(Dense(units=output_labels, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_6()\n",
    "print(\"Training...\")\n",
    "history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.3,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the change in the loss over the epochs.\n",
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss during training\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['acc'], label='train')\n",
    "pyplot.plot(history.history['val_acc'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(X_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.928482\n",
      "Precision: 0.928482\n",
      "Recall: 1.000000\n",
      "F1 score: 0.962915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes, average='weighted', labels=np.unique(yhat_classes))\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes, average='weighted', labels=np.unique(yhat_classes))\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes, average='weighted', labels=np.unique(yhat_classes))\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens kappa: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# kappa\n",
    "kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.496405\n"
     ]
    }
   ],
   "source": [
    "# ROC AUC\n",
    "auc = roc_auc_score(y_test, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "import seaborn as sns\n",
    "LABELS = [\"Normal\",\"Attack\"]\n",
    "\n",
    "matrix = confusion_matrix(y_test, yhat_classes)\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/Output Data to LSTM for Sequence Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "#https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=numpy.nan)\n",
    "\n",
    "def int_to_onehot(n, n_classes):\n",
    "    v = [0] * n_classes\n",
    "    v[n] = 1\n",
    "    return v\n",
    "\n",
    "def onehot_to_int(v):\n",
    "    return v.index(1)\n",
    "\n",
    "X_train, y_train, X_test, y_test\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(X_train[:1,:,:])\n",
    "\n",
    "# systemcall trace-1 length = 819, \n",
    "# [6, 6, 63, 6, 42, 120, 6, 195, 120, 6, 6, 114, 114, 1, 1, 252, 252,\n",
    "# 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252,\n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 1, 252, 1, 1, 1,\n",
    "# 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1,\n",
    "# 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 1, 1, 252, 1, 252, 252, 252, \n",
    "# 252, 252, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 252, 1, 1, 252, 1, 1, 252, 1, 1, 252, 252, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 252, 1, 1, 1, 1, 1, 1, 252, 252, 1, 1, 1, 1, 1, 252, 252, 252, 252, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 1, 252, 252, 1, 252, 252, 1, 1, 252, 252, 252, \n",
    "# 1, 1, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 1, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 252, 1, 1, 252, 1, 1, 252, 1, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 1, 252, 1, 1, 252, 1, 252, 252, 252, 1, 252, \n",
    "# 252, 252, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 1, 252]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Sequence [6, 6, 63, 6, 42, 120, 6, 195, 120, 6]\n",
    "# [X -> 6, 6, 63, 6, 42, 120, 6, 195, 120, Y-> 6]\n",
    "pprint.pprint(y_train[:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-7154605aa8d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# [X ->114, 162 ,114, 114 ,162, 114, 162  Y-> 162]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m test_input = array([[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n\u001b[0m\u001b[0;32m      6\u001b[0m          \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m          \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sequence [114 ,162, 114, 114 ,162, 114, 162, 162]\n",
    "# [X ->114, 162 ,114, 114 ,162, 114, 162  Y-> 162]\n",
    "\n",
    "test_input = array([[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0]]])\n",
    "\n",
    "test_input = test_input.reshape((1, 19, 341))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "# https://towardsdatascience.com/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
