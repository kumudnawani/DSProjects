{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System call Anomaly Detection- Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADFA Dataset Preprocessing:**\n",
    "\n",
    "    1. The system call language model estimates the probability distribution of the next call in a sequence given the sequence of previous calls. \n",
    "       \n",
    "    2. We assume that the host system generates a finite number of system calls. \n",
    "    \n",
    "    3. We index each system call by using an integer starting from 1 and denote the fixed set of all possible system calls in the system as S = {1, · · · , K}. Let x = x1x2 · · · xl(xi ∈ S) denote a sequence of l system calls.\n",
    "       \n",
    "**LSTM Based Model :**     \n",
    "\n",
    "    1. At the Input Layer, the call at each time step xi is fed into the model in the form of one-hot encoding,\n",
    "       in other words, a K dimensional vector with all elements zero except position xi.\n",
    "       \n",
    "    2. At the Embedding Layer*, incoming calls are embedded to continuous space by multiplying embedding matrix W,\n",
    "       which should be learned. \n",
    "       \n",
    "    3. At the Hidden Layer*, the LSTM unit has an internal state, and this state is updated recurrently at each time step.\n",
    "    \n",
    "    4. At the Output Layer, a softmax activation function is used to produce the estimation of normalized probability values of possible calls coming next in the sequence.\n",
    "    \n",
    "**References for systemcalls:**\n",
    "    1. http://osinside.net/syscall/system_call_table.htm\n",
    "    2. https://www.cs.unm.edu/~immsec/systemcalls.htm    \n",
    "    3. https://github.com/karpathy/char-rnn\n",
    "    4. https://keras.io/losses/#categorical_crossentropy\n",
    "    5. http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADFA Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  1 13:52:35 2019\n",
    "\n",
    "@author: kuna\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# ignore all user warnings\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "def saveintopickle(obj, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print (\"[Pickle]: save object into {}\".format(filename))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def loadfrompickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "\n",
    "\n",
    "#draw the  process bar\n",
    "def drawProgressBar(percent, barLen = 20):\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        else:\n",
    "            progress += \" \"\n",
    "    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import io_helper\n",
    "\n",
    "\n",
    "random_data_dup = 10  # each sample randomly duplicated between 0 and 9 times, see dropin function\n",
    "\n",
    "\n",
    "def dropin(X, y):\n",
    "    \"\"\"\n",
    "    The name suggests the inverse of dropout, i.e. adding more samples. See Data Augmentation section at\n",
    "    http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/\n",
    "    :param X: Each row is a training sequence\n",
    "    :param y: Tne target we train and will later predict\n",
    "    :return: new augmented X, y\n",
    "    \"\"\"\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    X_hat = []\n",
    "    y_hat = []\n",
    "    for i in range(0, len(X)):\n",
    "        for j in range(0, np.random.random_integers(0, random_data_dup)):\n",
    "            X_hat.append(X[i, :])\n",
    "            y_hat.append(y[i])\n",
    "    return np.asarray(X_hat), np.asarray(y_hat)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    arrayfile = \"./array_test.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_train = array[:,:-1]\n",
    "    y_train = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_train.shape)\n",
    "    print (y_train.shape)\n",
    "    return (x_train,y_train)\n",
    "\n",
    "def preprocess_val():\n",
    "\n",
    "    arrayfile = \"./array_val.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_test = array[:,:-1]\n",
    "    y_test = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_test.shape)\n",
    "    print (y_test.shape)\n",
    "    return (x_test,y_test)\n",
    "\n",
    "#if __name__ ==\"__main__\":\n",
    "#   preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834\n",
      "Skip the file ADFA-LD/Training_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 63, 64, 65, 66, 75, 77, 78, 83, 85, 91, 93, 94, 96, 97, 99, 102, 104, 110, 114, 117, 118, 119, 120, 122, 125, 128, 132, 133, 140, 141, 142, 143, 144, 146, 148, 155, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 179, 180, 183, 184, 185, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 219, 220, 221, 224, 226, 228, 229, 230, 231, 233, 234, 240, 242, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 298, 300, 301, 307, 308, 309, 311, 314, 320, 322, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "833\n",
      "The maximum length of a sequence is that 2948\n",
      "lists_of_list_into_big_matrix\n",
      "833\n",
      "[ =                    ] 8.52%(20298, 20, 341)\n",
      "done\n",
      "[Pickle]: save object into array_test.pickle\n",
      "4373\n",
      "Skip the file ADFA-LD/Validation_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 22, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 61, 63, 64, 65, 66, 75, 77, 78, 79, 83, 85, 90, 91, 93, 94, 96, 97, 99, 102, 104, 110, 111, 114, 116, 117, 118, 119, 120, 122, 124, 125, 128, 132, 133, 136, 140, 141, 142, 143, 144, 146, 148, 150, 151, 154, 155, 156, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 190, 191, 192, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 224, 226, 228, 229, 231, 234, 240, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 296, 298, 300, 301, 306, 307, 308, 309, 311, 314, 320, 324, 328, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "4372\n",
      "The maximum length of a sequence is that 4494\n",
      "[                      ] 1.26%(21238, 20, 341)\n",
      "done\n",
      "[Pickle]: save object into array_val.pickle\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "#import io_helper\n",
    "\n",
    "def readfilesfromAdir(dataset):\n",
    "    #read a list of files\n",
    "    files = os.listdir(dataset)\n",
    "    files_absolute_paths = []\n",
    "    for i in files:\n",
    "        files_absolute_paths.append(dataset+str(i))\n",
    "    return files_absolute_paths\n",
    "\n",
    "\n",
    "file = \"ADFA-LD/Training_Data_Master/UTD-0001.txt\"\n",
    "#this is used to read a char sequence from\n",
    "def readCharsFromFile(file):\n",
    "    channel_values = open(file).read().split()\n",
    "    #print (len(channel_values))\n",
    "    #channel_values is a list\n",
    "    return channel_values\n",
    "    #print (channel_values[800:819])\n",
    "\n",
    "def get_attack_subdir(path):\n",
    "    subdirectories = os.listdir(path)\n",
    "    for i in range(0,len(subdirectories)):\n",
    "        subdirectories[i] = path + subdirectories[i]\n",
    "\n",
    "    print (subdirectories)\n",
    "    return (subdirectories)\n",
    "\n",
    "\n",
    "def get_all_call_sequences(dire):\n",
    "    files = readfilesfromAdir(dire)\n",
    "    allthelist = []\n",
    "    print (len(files))\n",
    "\n",
    "    for eachfile in files:\n",
    "        if not eachfile.endswith(\"DS_Store\"):\n",
    "            allthelist.append(readCharsFromFile(eachfile))\n",
    "        else:\n",
    "            print (\"Skip the file \"+ str(eachfile))\n",
    "\n",
    "    elements = []\n",
    "    for item in allthelist:\n",
    "        for key in item:\n",
    "            if key not in elements:\n",
    "                elements.append(key)\n",
    "\n",
    "    elements = map(int,elements)\n",
    "    elements = sorted(elements)\n",
    "\n",
    "    print (\"The total unique elements:\")\n",
    "    print (elements)\n",
    "\n",
    "    print (\"The maximum number of elements:\")\n",
    "    print (max(elements))\n",
    "\n",
    "    #print (\"The length elements:\")\n",
    "    #print (len(elements))\n",
    "    print (len(allthelist))\n",
    "\n",
    "    #clean the all list data set\n",
    "    _max = 0\n",
    "    for i in range(0,len(allthelist)):\n",
    "        _max = max(_max,len(allthelist[i]))\n",
    "        allthelist[i] = list(map(int,allthelist[i]))\n",
    "        #print(allthelist[i])\n",
    "\n",
    "\n",
    "    print (\"The maximum length of a sequence is that {}\".format(_max))\n",
    "\n",
    "    return (allthelist)\n",
    "\n",
    "## shift the data for analysis\n",
    "def shift(seq, n):\n",
    "    n = n % len(seq)\n",
    "    return seq[n:] + seq[:n]\n",
    "\n",
    "\n",
    "def convertToOneHot(vector, num_classes=None):\n",
    "    \"\"\"\n",
    "    Converts an input 1-D vector of integers into an output\n",
    "    2-D array of one-hot vectors, where an i'th input value\n",
    "    of j will set a '1' in the i'th row, j'th column of the\n",
    "    output array.\n",
    "\n",
    "    Example:\n",
    "        v = np.array((1, 0, 4))\n",
    "        one_hot_v = convertToOneHot(v)\n",
    "        print one_hot_v\n",
    "\n",
    "        [[0 1 0 0 0]\n",
    "         [1 0 0 0 0]\n",
    "         [0 0 0 0 1]]\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(vector, np.ndarray)\n",
    "    assert len(vector) > 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(vector)+1\n",
    "    else:\n",
    "        assert num_classes > 0\n",
    "        assert num_classes >= np.max(vector)\n",
    "\n",
    "    result = np.zeros(shape=(len(vector), num_classes))\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result.astype(int)\n",
    "\n",
    "\"\"\"\n",
    "The num_class here is set as 341\n",
    "\"\"\"\n",
    "\n",
    "#one function do one thing\n",
    "def sequence_n_gram_parsing(alist,n_gram=20,num_class=341):\n",
    "    if len(alist) <= n_gram:\n",
    "        return alist\n",
    "\n",
    "    ans = []\n",
    "    for i in range(0,len(alist)-n_gram+1,1):\n",
    "        tmp = alist[i:i+n_gram]\n",
    "        oneHot = convertToOneHot(np.asarray(tmp), num_class)\n",
    "        #print(tmp)\n",
    "        #print(np.asarray(tmp))\n",
    "        #print(oneHot)\n",
    "        ans.append(oneHot)\n",
    "\n",
    "    #transform into nmup arrray\n",
    "    ans = np.array(ans)\n",
    "    return (ans)\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix(allthelist,n_gram=20):\n",
    "    \n",
    "    print(\"lists_of_list_into_big_matrix\")\n",
    "    print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "    #print(len(allthelist[0]))\n",
    "    #print(allthelist[0])\n",
    "    #print(len(array))\n",
    "    #print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "       \n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "       \n",
    "        #print (\"tmp shape\")\n",
    "        #print(tmp)\n",
    "        #print (len(tmp))\n",
    " \n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "        #print(allthelist[i])\n",
    "        #print(array)\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "        #print(len(allthelist[1]))\n",
    "        #print(allthelist[1])\n",
    "        #print(len(array))\n",
    "        #print(array)\n",
    "        #break\n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_test.pickle\")\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix_val(allthelist,n_gram=20):\n",
    "\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_val.pickle\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dirc = \"ADFA-LD/Training_Data_Master/\"\n",
    "    dirc_val = \"ADFA-LD/Validation_Data_Master/\"\n",
    "    dic_attack =\"ADFA-LD/Attack_Data_Master/Adduser_1/\"\n",
    "    #train1 = get_all_call_sequences(dirc)\n",
    "\n",
    "    #test = [i for i in range(0,300)]\n",
    "    #array = sequence_n_gram_parsing(test)\n",
    "    #print (type(array))\n",
    "    #print (array.shape)\n",
    "\n",
    "    #get_attack_subdir(dic_attack)\n",
    "    #print (\"XxxxxxxXXXXXXXXXXX\")\n",
    "    #val1 = get_all_call_sequences(dirc_val)\n",
    "    \n",
    "    #dirc_test = \"Test/\"\n",
    "    #att_test = get_all_call_sequences(dirc_test)\n",
    "    #lists_of_list_into_big_matrix(att_test)\n",
    "    \n",
    "    att = get_all_call_sequences(dirc)\n",
    "    lists_of_list_into_big_matrix(att)\n",
    "    \n",
    "    att_val = get_all_call_sequences(dirc_val)\n",
    "    lists_of_list_into_big_matrix_val(att_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "#import preprocess\n",
    "\n",
    "# Global hyper-parameters\n",
    "sequence_length = 19\n",
    "epochs = 1\n",
    "batch_size = 50\n",
    "feature_dimension = 341\n",
    "top_words = 5000\n",
    "\n",
    "def save_model_weight_into_file(model, modelname=\"model.json\", weight=\"model.h5\"):\n",
    "    model_json = model.to_json()\n",
    "    with open(modelname, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(weight)\n",
    "    print(\"Saved model to disk in {} and {}\".format(modelname,weight))\n",
    "\n",
    "\n",
    "def load_model_and_wieght_from_file(modelname=\"model.json\", weight=\"model.h5\"):\n",
    "\n",
    "    json_file = open(modelname, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weight)\n",
    "    print(\"Loaded model from disk, you can do more analysis more\")\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    layers = {'input': feature_dimension, 'hidden1': 64, 'hidden2': 256, 'hidden3': 100, 'output': feature_dimension}\n",
    "\n",
    "    model.add(LSTM(\n",
    "            input_length=sequence_length,\n",
    "            input_dim=layers['input'],\n",
    "            output_dim=layers['hidden1'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden2'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden3'],\n",
    "            return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "            output_dim=layers['output'],activation='softmax'))\n",
    "    #model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop',  metrics=['accuracy'])\n",
    "    #model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def run_network(model=None, data=None):\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    \n",
    "    if data is None:\n",
    "        print ('Loading data... ')\n",
    "        # train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "        X_train, y_train  = preprocess()\n",
    "    else:\n",
    "        X_train, y_train = data\n",
    "\n",
    "    print (\"X_train, y_train,shape\")\n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "    if model is None:\n",
    "        model = build_model()\n",
    "        #model = build_model_2()\n",
    "        print(\"Training...\")\n",
    "        model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_split=0.3)\n",
    "        model.summary()\n",
    "        print(\"Done Training...\")\n",
    "\n",
    "    #predicted = model.predict(X_test)\n",
    "    #print(\"Reshaping predicted\")\n",
    "    #predicted = np.reshape(predicted, (predicted.size,))\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"prediction exception\")\n",
    "        print 'Training duration (s) : ', time.time() - global_start_time\n",
    "        return model, y_test, 0\n",
    "   \n",
    "    try:\n",
    "        plt.figure(1)\n",
    "        plt.subplot(311)\n",
    "        plt.title(\"Actual Test Signal w/Anomalies\")\n",
    "        plt.plot(y_test[:len(y_test)], 'b')\n",
    "        plt.subplot(312)\n",
    "        plt.title(\"Predicted Signal\")\n",
    "        plt.plot(predicted[:len(y_test)], 'g')\n",
    "        plt.subplot(313)\n",
    "        plt.title(\"Squared Error\")\n",
    "        mse = ((y_test - predicted) ** 2)\n",
    "        plt.plot(mse, 'r')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"plotting exception\")\n",
    "        print (str(e))\n",
    "    print ('Training duration (s) : '% (time.time() - global_start_time))\n",
    "\n",
    "    return model, y_test, predicted\n",
    "   \"\"\"\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "# run_network()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kuna\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\kuna\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Training...\n",
      "WARNING:tensorflow:From C:\\Users\\kuna\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 14208 samples, validate on 6090 samples\n",
      "Epoch 1/1\n",
      "14208/14208 [==============================] - 32s 2ms/step - loss: 2.7770 - acc: 0.2367 - val_loss: 2.8779 - val_acc: 0.2154\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 19, 64)            103936    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 19, 256)           328704    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 19, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               142800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 341)               34441     \n",
      "=================================================================\n",
      "Total params: 609,881\n",
      "Trainable params: 609,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "if model is None:\n",
    "    model = build_model()\n",
    "    print(\"Training...\")\n",
    "    history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.3,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    model.summary()\n",
    "    print(\"Done Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "#def loadData(file): \n",
    "    # for reading also binary mode is important \n",
    "#    dbfile = open(file, 'rb')      \n",
    "#    db = pickle.load(dbfile) \n",
    "#    for keys in db: \n",
    "#        print(keys, '=>', db[keys]) \n",
    "#    dbfile.close() \n",
    "  \n",
    "#if __name__ == '__main__': \n",
    "#    loadData(\"./array_test.pickle\") \n",
    "#df_val = pd.read_pickle(\"./array_val.pickle\")\n",
    "#df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data size is that \n",
      "(21238, 19, 341)\n",
      "(21238, 341)\n",
      "X_test, y_test,shape\n",
      "(21238, 19, 341)\n",
      "(21238, 341)\n",
      "Validating...\n",
      "Done Validating...\n",
      "[[2.8147128e-05 3.8867351e-02 5.9301241e-05 ... 3.2527638e-05\n",
      "  4.0639268e-05 6.0572522e-04]\n",
      " [2.7337572e-05 4.2425249e-02 5.7118334e-05 ... 3.0709063e-05\n",
      "  3.9311104e-05 5.8961258e-04]\n",
      " [3.2080068e-05 4.1573644e-02 6.1957377e-05 ... 3.6363443e-05\n",
      "  4.3218708e-05 6.0780835e-04]\n",
      " ...\n",
      " [1.8595829e-06 1.2511486e-03 3.5294606e-06 ... 2.1812916e-06\n",
      "  1.6237399e-06 6.7461682e-05]\n",
      " [1.8240867e-06 1.3079355e-03 3.5116327e-06 ... 2.1674750e-06\n",
      "  1.6114174e-06 6.7553679e-05]\n",
      " [1.8013474e-06 1.3025296e-03 3.4824586e-06 ... 2.1350809e-06\n",
      "  1.6007832e-06 6.6800356e-05]]\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "X_test, y_test = preprocess_val()\n",
    "\n",
    "print (\"X_test, y_test,shape\")\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "print(\"Validating...\")\n",
    "predicted = model.predict(X_test)\n",
    "print(\"Done Validating...\")\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did our model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 3.08\n",
      "Validation Accuracy : 0.19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Score : %.2f'%(score))\n",
    "print('Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Loss')\n",
    "#plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [2.8778519998434535],\n",
       " 'val_acc': [0.21543513882556573],\n",
       " 'loss': [2.776978516363883],\n",
       " 'acc': [0.23669763501452468]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Accuracy')\n",
    "#plt.plot(history.history['acc'], label='train')\n",
    "#plt.plot(history.history['val_acc'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Test with new systemcall  sequence ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hidden nodes is 100.\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "\n",
    "word_vec_length = 19\n",
    "char_vec_length = 341\n",
    "output_labels = 341\n",
    "\n",
    "\n",
    "#hidden_nodes = 4000 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "hidden_nodes = 100\n",
    "print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "def build_model_2():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 2.8397 - acc: 0.2396 - val_loss: 2.5586 - val_acc: 0.4236\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.9795 - acc: 0.4184 - val_loss: 2.1248 - val_acc: 0.4901\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.6778 - acc: 0.5024 - val_loss: 2.0376 - val_acc: 0.5023\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.5040 - acc: 0.5570 - val_loss: 1.9283 - val_acc: 0.5027\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 20s 977us/step - loss: 1.3988 - acc: 0.5863 - val_loss: 1.9624 - val_acc: 0.5107\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 20s 1ms/step - loss: 1.3225 - acc: 0.6038 - val_loss: 1.9617 - val_acc: 0.5203\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.2664 - acc: 0.6213 - val_loss: 1.9898 - val_acc: 0.5073\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 20s 998us/step - loss: 1.2196 - acc: 0.6373 - val_loss: 2.0098 - val_acc: 0.5102\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 1.1759 - acc: 0.6471 - val_loss: 2.0388 - val_acc: 0.5134\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 20s 980us/step - loss: 1.1451 - acc: 0.6549 - val_loss: 2.0311 - val_acc: 0.5157\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_2()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score : 1.05\n",
      "Train Validation Accuracy : 0.68\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_train, y_train, verbose=2, batch_size=batch_size)\n",
    "print('Train Score : %.2f'%(score))\n",
    "print('Train Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score : 2.03\n",
      "Test Validation Accuracy : 0.52\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Test Score : %.2f'%(score))\n",
    "print('Test Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## k-fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train\n",
    "Y = y_train\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 10-fold cross validation test harness\n",
    "#kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#cvscores = []\n",
    "#for train, test in kfold.split(X, Y):\n",
    "#  # create model\n",
    "#\tmodel = Sequential()\n",
    "#\tmodel.add(Dense(12, input_dim=341, activation='relu'))\n",
    "#\tmodel.add(Dense(8, activation='relu'))\n",
    "#\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "#\t# Compile model\n",
    "#\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\t# Fit the model\n",
    "#\tmodel.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "#\t# evaluate the model\n",
    "#\tscores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "#\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#\tcvscores.append(scores[1] * 100)\n",
    "#print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hidden nodes is 100.\n",
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 24s 1ms/step - loss: 0.0118 - acc: 0.9971 - val_loss: 0.0100 - val_acc: 0.9973\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 20s 1ms/step - loss: 0.0086 - acc: 0.9973 - val_loss: 0.0086 - val_acc: 0.9976 0.99 - ETA: 5s - loss: 0. - ETA: 4s - loss: 0.00 - ETA: 3s - loss: 0.0087 - acc: 0.9 - ETA: 3s - loss: 0.0087 - - ETA: 2s - loss: 0. - ETA: 1s - loss: 0.\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 23s 1ms/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0083 - val_acc: 0.9977\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 24s 1ms/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0078 - val_acc: 0.9977\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 20s 1ms/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0077 - val_acc: 0.9978\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 20s 994us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "\n",
    "word_vec_length = 19\n",
    "char_vec_length = 341\n",
    "output_labels = 341\n",
    "\n",
    "\n",
    "hidden_nodes = 100 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "def build_model_3():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_3()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 23s 1ms/step - loss: 0.0112 - acc: 0.9971 - val_loss: 0.0099 - val_acc: 0.9975\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 20s 986us/step - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0085 - val_acc: 0.9977\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 20s 996us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0078 - val_acc: 0.9977\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0079 - val_acc: 0.9977\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 20s 994us/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "def build_model_4():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_4()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 28s 1ms/step - loss: 0.0111 - acc: 0.9971 - val_loss: 0.0100 - val_acc: 0.9973\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.0085 - val_acc: 0.9977\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0081 - val_acc: 0.9978\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0079 - val_acc: 0.9978\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0081 - val_acc: 0.9977\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 22s 1ms/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0081 - val_acc: 0.9977\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 21s 1ms/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0082 - val_acc: 0.9978\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "def build_model_5():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_5()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score : 0.00\n",
      "Train Validation Accuracy : 1.00\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_train, y_train, verbose=2, batch_size=batch_size)\n",
    "print('Train Score : %.2f'%(score))\n",
    "print('Train Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score : 0.01\n",
      "Test Validation Accuracy : 1.00\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Test Score : %.2f'%(score))\n",
    "print('Test Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM for Binary Classification of SystemCalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834\n",
      "Skip the file ADFA-LD/Training_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 63, 64, 65, 66, 75, 77, 78, 83, 85, 91, 93, 94, 96, 97, 99, 102, 104, 110, 114, 117, 118, 119, 120, 122, 125, 128, 132, 133, 140, 141, 142, 143, 144, 146, 148, 155, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 179, 180, 183, 184, 185, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 219, 220, 221, 224, 226, 228, 229, 230, 231, 233, 234, 240, 242, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 298, 300, 301, 307, 308, 309, 311, 314, 320, 322, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "833\n",
      "The maximum length of a sequence is that 2948\n",
      "[ ==                   ] 14.53%(40099, 20)\n",
      "done\n",
      "[Pickle]: save object into array_test.pickle\n",
      "4373\n",
      "Skip the file ADFA-LD/Validation_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 22, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 61, 63, 64, 65, 66, 75, 77, 78, 79, 83, 85, 90, 91, 93, 94, 96, 97, 99, 102, 104, 110, 111, 114, 116, 117, 118, 119, 120, 122, 124, 125, 128, 132, 133, 136, 140, 141, 142, 143, 144, 146, 148, 150, 151, 154, 155, 156, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 190, 191, 192, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 224, 226, 228, 229, 231, 234, 240, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 296, 298, 300, 301, 306, 307, 308, 309, 311, 314, 320, 324, 328, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "4372\n",
      "The maximum length of a sequence is that 4494\n",
      "[                      ] 2.13%(40142, 20)\n",
      "done\n",
      "[Pickle]: save object into array_val.pickle\n",
      "The total unique elements:\n",
      "[3, 4, 5, 6, 7, 13, 19, 33, 43, 45, 54, 60, 78, 91, 102, 104, 119, 120, 122, 140, 142, 146, 162, 168, 175, 183, 192, 195, 196, 197, 220, 221, 240, 265, 268, 292, 331, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "16\n",
      "The maximum length of a sequence is that 2161\n",
      "[ ==================   ] 93.75%(6184, 20)\n",
      "done\n",
      "[Pickle]: save object into array_attack.pickle\n",
      "(40099, 20)\n",
      "The train data size is that \n",
      "(40099, 20, 1)\n",
      "(40099, 1)\n",
      "(40142, 20)\n",
      "The validation data size is that \n",
      "(40142, 20, 1)\n",
      "(40142, 1)\n",
      "The attack data size is that \n",
      "(6184, 20, 1)\n",
      "(6184, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess():\n",
    "\n",
    "    arrayfile = \"./array_test.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_train = array[:,:]\n",
    "    print (x_train.shape)\n",
    "    x_train = x_train.reshape(40099, 20, 1)\n",
    "    y_train = np.zeros((40099,1))\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_train.shape)\n",
    "    print (y_train.shape)\n",
    "    return (x_train,y_train)\n",
    "\n",
    "def preprocess_val():\n",
    "\n",
    "    arrayfile = \"./array_val.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_test = array[:,:]\n",
    "    print (x_test.shape)\n",
    "    x_test = x_test.reshape(40142, 20, 1)\n",
    "    y_test = np.zeros((40142,1))\n",
    "\n",
    "    print (\"The validation data size is that \")\n",
    "    print (x_test.shape)\n",
    "    print (y_test.shape)\n",
    "    return (x_test,y_test)\n",
    "\n",
    "def preprocess_attack():\n",
    "\n",
    "    arrayfile = \"./array_attack.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_attack = array[:,:]\n",
    "    x_attack = x_attack.reshape(6184, 20, 1)\n",
    "    y_attack = np.ones((6184,1))\n",
    "\n",
    "    print (\"The attack data size is that \")\n",
    "    print (x_attack.shape)\n",
    "    print (y_attack.shape)\n",
    "    return (x_attack,y_attack)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The num_class here is set as 1\n",
    "\"\"\"\n",
    "\n",
    "#one function do one thing\n",
    "def sequence_n_gram_parsing_noencoding(alist,n_gram=20,num_class=1):\n",
    "    if len(alist) <= n_gram:\n",
    "        return alist\n",
    "\n",
    "    ans = []\n",
    "    for i in range(0,len(alist)-n_gram+1,1):\n",
    "        tmp = alist[i:i+n_gram]\n",
    "        #oneHot = convertToOneHot(np.asarray(tmp), num_class)\n",
    "        #print(tmp)\n",
    "        #print(np.asarray(tmp))\n",
    "        #print(oneHot)\n",
    "        ans.append(tmp)\n",
    "\n",
    "    #transform into nmup arrray\n",
    "    ans = np.array(ans)\n",
    "    return (ans)\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix(allthelist,n_gram=20):\n",
    "    \n",
    "    #print(\"lists_of_list_into_big_matrix train\")\n",
    "    #print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing_noencoding(allthelist[0])\n",
    "    #print(len(allthelist[0]))\n",
    "    #print(allthelist[0])\n",
    "    #print(len(array))\n",
    "    #print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "       \n",
    "        tmp = sequence_n_gram_parsing_noencoding(allthelist[i])\n",
    "       \n",
    "        #print (\"tmp shape\")\n",
    "        #print(tmp.shape)\n",
    "        #print(array.shape)\n",
    "        #print (len(tmp))\n",
    " \n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "        #print(allthelist[i])\n",
    "        #print(array)\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 40000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "        #print(len(allthelist[1]))\n",
    "        #print(allthelist[1])\n",
    "        #print(len(array))\n",
    "        #print(array)\n",
    "        #break\n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_test.pickle\")\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix_val(allthelist,n_gram=20):\n",
    "\n",
    "    #print(\"lists_of_list_into_big_matrix validation\")\n",
    "    #print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing_noencoding(allthelist[0])\n",
    "    #print(len(allthelist[0]))\n",
    "    #print(allthelist[0])\n",
    "    #print(len(array))\n",
    "    #print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing_noencoding(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 40000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_val.pickle\")\n",
    "\n",
    "def get_all_call_sequences_attack(dire):\n",
    "    # list of attacks\n",
    "    attack = ['Adduser','Hydra_FTP','Hydra_SSH','Java_Meterpreter','Meterpreter','Web_Shell']\n",
    "    #attack = ['Adduser' ,'Hydra_FTP']\n",
    "    for term in attack:\n",
    "        in_address = dire+term          \n",
    "        for i in range (1,11):\n",
    "            files = readfilesfromAdir(in_address+\"_\"+str(i)+\"/\")\n",
    "        \n",
    "    allthelist = []\n",
    "    #print(files)\n",
    "    #print (len(files))\n",
    "\n",
    "    for eachfile in files:\n",
    "        if not eachfile.endswith(\"DS_Store\"):\n",
    "            allthelist.append(readCharsFromFile(eachfile))\n",
    "        else:\n",
    "            print (\"Skip the file \"+ str(eachfile))\n",
    "\n",
    "    elements = []\n",
    "    for item in allthelist:\n",
    "        for key in item:\n",
    "            if key not in elements:\n",
    "                elements.append(key)\n",
    "\n",
    "    elements = map(int,elements)\n",
    "    elements = sorted(elements)\n",
    "\n",
    "    print (\"The total unique elements:\")\n",
    "    print (elements)\n",
    "\n",
    "    print (\"The maximum number of elements:\")\n",
    "    print (max(elements))\n",
    "\n",
    "    #print (\"The length elements:\")\n",
    "    #print (len(elements))\n",
    "    print (len(allthelist))\n",
    "\n",
    "    #clean the all list data set\n",
    "    _max = 0\n",
    "    for i in range(0,len(allthelist)):\n",
    "        _max = max(_max,len(allthelist[i]))\n",
    "        allthelist[i] = list(map(int,allthelist[i]))\n",
    "        #print(allthelist[i])\n",
    "\n",
    "\n",
    "    print (\"The maximum length of a sequence is that {}\".format(_max))\n",
    "\n",
    "    return (allthelist)\n",
    "\n",
    "def lists_of_list_into_big_matrix_attack(allthelist,n_gram=20):\n",
    "\n",
    "    array = sequence_n_gram_parsing_noencoding(allthelist[0])\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing_noencoding(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "        #print (tmp.shape)\n",
    "        #print (array.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 40000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_attack.pickle\")\n",
    "    #pickle2csv(\"array_attack.pickle\", \"attack.csv\")\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    dirc = \"ADFA-LD/Training_Data_Master/\"\n",
    "    dirc_val = \"ADFA-LD/Validation_Data_Master/\"\n",
    "    dic_attack =\"ADFA-LD/Attack_Data_Master/\"\n",
    "    \n",
    "    att = get_all_call_sequences(dirc)\n",
    "    lists_of_list_into_big_matrix(att)\n",
    "    \n",
    "    att_val = get_all_call_sequences(dirc_val)\n",
    "    lists_of_list_into_big_matrix_val(att_val)\n",
    "    \n",
    "    att_attack = get_all_call_sequences_attack(dic_attack)\n",
    "    lists_of_list_into_big_matrix_attack(att_attack)\n",
    "    \n",
    "    test_split = 0.2\n",
    "    \n",
    "    X_train_p, y_train_p = preprocess()\n",
    "    \n",
    "    X_test_p, y_test_p = preprocess_val()\n",
    "\n",
    "    X_attack_p, y_attack_p = preprocess_attack()\n",
    "    \n",
    "    X_a1, X_a2 = np.array_split(X_attack_p, 2)\n",
    "    y_a1, y_a2 = np.array_split(y_attack_p, 2)\n",
    "    \n",
    "    X_train = np.concatenate([X_train_p, X_a1])\n",
    "    y_train = np.concatenate([y_train_p, y_a1])\n",
    "    \n",
    "    X_test = np.concatenate([X_test_p, X_a2])\n",
    "    y_test = np.concatenate([y_test_p, y_a2])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = np.concatenate((X_train, y_train[:,None]), axis=1)#np.random.shuffle\n",
    "X_train_t.shape\n",
    "np.random.shuffle(X_train_t)\n",
    "y_train = X_train_t[:,-1]\n",
    "X_train = X_train_t[:,:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = np.concatenate((X_test, y_test[:,None]), axis=1)#np.random.shuffle\n",
    "X_test_t.shape\n",
    "np.random.shuffle(X_test_t)\n",
    "y_test = X_test_t[:,-1]\n",
    "X_test = X_test_t[:,:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 33.],\n",
      "       [192.],\n",
      "       [  6.],\n",
      "       [ 33.],\n",
      "       [  6.],\n",
      "       [192.],\n",
      "       [125.],\n",
      "       [197.],\n",
      "       [197.],\n",
      "       [197.],\n",
      "       [197.],\n",
      "       [ 85.],\n",
      "       [ 85.],\n",
      "       [174.],\n",
      "       [174.],\n",
      "       [174.],\n",
      "       [174.],\n",
      "       [195.],\n",
      "       [  3.],\n",
      "       [  3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43191, 20, 1)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint.pprint(X_train_t[0,:20,:])\n",
    "type(X_train_t[:,:20,:])\n",
    "X_train_t[:,:20,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43191, 20, 1),\n",
       " (43191, 1),\n",
       " (43234, 20, 1),\n",
       " (43234, 1),\n",
       " (6184, 20, 1),\n",
       " (6184, 1))"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape , X_attack.shape, y_attack.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "X_train, y_train,shape\n",
      "(43191, 20, 1)\n",
      "(43191, 1)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 42759 samples, validate on 432 samples\n",
      "Epoch 1/10\n",
      "42759/42759 [==============================] - 27s 627us/step - loss: 0.2558 - acc: 0.8988 - val_loss: 0.1816 - val_acc: 0.9306\n",
      "Epoch 2/10\n",
      "42759/42759 [==============================] - 21s 496us/step - loss: 0.1651 - acc: 0.9272 - val_loss: 0.1562 - val_acc: 0.9329\n",
      "Epoch 3/10\n",
      "42759/42759 [==============================] - 22s 503us/step - loss: 0.1518 - acc: 0.9305 - val_loss: 0.1500 - val_acc: 0.9329\n",
      "Epoch 4/10\n",
      "42759/42759 [==============================] - 22s 510us/step - loss: 0.1445 - acc: 0.9324 - val_loss: 0.1481 - val_acc: 0.9306\n",
      "Epoch 5/10\n",
      "42759/42759 [==============================] - 22s 512us/step - loss: 0.1380 - acc: 0.9352 - val_loss: 0.1405 - val_acc: 0.9375\n",
      "Epoch 6/10\n",
      "42759/42759 [==============================] - 22s 520us/step - loss: 0.1408 - acc: 0.9372 - val_loss: 0.1384 - val_acc: 0.9421\n",
      "Epoch 7/10\n",
      "42759/42759 [==============================] - 22s 523us/step - loss: 0.1306 - acc: 0.9395 - val_loss: 0.1367 - val_acc: 0.9444\n",
      "Epoch 8/10\n",
      "42759/42759 [==============================] - 22s 509us/step - loss: 0.1261 - acc: 0.9411 - val_loss: 0.1205 - val_acc: 0.9468\n",
      "Epoch 9/10\n",
      "42759/42759 [==============================] - 22s 523us/step - loss: 0.1213 - acc: 0.9440 - val_loss: 0.1112 - val_acc: 0.9560\n",
      "Epoch 10/10\n",
      "42759/42759 [==============================] - 22s 515us/step - loss: 0.1131 - acc: 0.9458 - val_loss: 0.1089 - val_acc: 0.9560\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_41 (LSTM)               (None, 13)                780       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 14        \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 796\n",
      "Trainable params: 796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "word_vec_length = 20\n",
    "char_vec_length = 1\n",
    "output_labels = 1\n",
    "hidden_nodes = 13 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def build_model_6():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    #model.add(Activation('softmax'))\n",
    "    model.add(Dense(units=output_labels, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_6()\n",
    "print(\"Training...\")\n",
    "history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.01,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the change in the loss over the epochs.\n",
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss during training\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Loss')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy during training\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['acc'], label='train')\n",
    "pyplot.plot(history.history['val_acc'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(X_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     39123\n",
      "           1       0.20      0.15      0.17      4111\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     43234\n",
      "   macro avg       0.56      0.54      0.55     43234\n",
      "weighted avg       0.85      0.86      0.85     43234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(yhat_classes, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.862261\n",
      "Precision: 0.880766\n",
      "Recall: 0.862261\n",
      "F1 score: 0.871119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(y_test, yhat_classes, average='weighted', labels=np.unique(yhat_classes))\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(y_test, yhat_classes, average='weighted', labels=np.unique(yhat_classes))\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(y_test, yhat_classes, average='weighted', labels=np.unique(yhat_classes))\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens kappa: 0.099771\n"
     ]
    }
   ],
   "source": [
    "# kappa\n",
    "kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "print('Cohens kappa: %f' % kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.601735\n"
     ]
    }
   ],
   "source": [
    "# ROC AUC\n",
    "auc = roc_auc_score(y_test, yhat_probs)\n",
    "print('ROC AUC: %f' % auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "import seaborn as sns\n",
    "LABELS = [\"Normal\",\"Attack\"]\n",
    "\n",
    "matrix = confusion_matrix(y_test, yhat_classes)\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/Output Data to LSTM for Sequence Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "#https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=numpy.nan)\n",
    "\n",
    "def int_to_onehot(n, n_classes):\n",
    "    v = [0] * n_classes\n",
    "    v[n] = 1\n",
    "    return v\n",
    "\n",
    "def onehot_to_int(v):\n",
    "    return v.index(1)\n",
    "\n",
    "X_train, y_train, X_test, y_test\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(X_train[:1,:,:])\n",
    "\n",
    "# systemcall trace-1 length = 819, \n",
    "# [6, 6, 63, 6, 42, 120, 6, 195, 120, 6, 6, 114, 114, 1, 1, 252, 252,\n",
    "# 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252,\n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 1, 252, 1, 1, 1,\n",
    "# 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1,\n",
    "# 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 1, 1, 252, 1, 252, 252, 252, \n",
    "# 252, 252, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 252, 1, 1, 252, 1, 1, 252, 1, 1, 252, 252, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 252, 1, 1, 1, 1, 1, 1, 252, 252, 1, 1, 1, 1, 1, 252, 252, 252, 252, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 1, 252, 252, 1, 252, 252, 1, 1, 252, 252, 252, \n",
    "# 1, 1, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 1, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 252, 1, 1, 252, 1, 1, 252, 1, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 1, 252, 1, 1, 252, 1, 252, 252, 252, 1, 252, \n",
    "# 252, 252, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 1, 252]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Sequence [6, 6, 63, 6, 42, 120, 6, 195, 120, 6]\n",
    "# [X -> 6, 6, 63, 6, 42, 120, 6, 195, 120, Y-> 6]\n",
    "pprint.pprint(y_train[:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-7154605aa8d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# [X ->114, 162 ,114, 114 ,162, 114, 162  Y-> 162]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m test_input = array([[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n\u001b[0m\u001b[0;32m      6\u001b[0m          \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m          \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'array' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sequence [114 ,162, 114, 114 ,162, 114, 162, 162]\n",
    "# [X ->114, 162 ,114, 114 ,162, 114, 162  Y-> 162]\n",
    "\n",
    "test_input = array([[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0]]])\n",
    "\n",
    "test_input = test_input.reshape((1, 19, 341))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
    "# https://towardsdatascience.com/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
