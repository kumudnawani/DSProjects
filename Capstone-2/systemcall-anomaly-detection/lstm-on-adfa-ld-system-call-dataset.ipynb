{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System call Anomaly Detection- Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADFA Dataset Preprocessing:**\n",
    "\n",
    "    1. The system call language model estimates the probability distribution of the next call in a sequence given the sequence of previous calls. \n",
    "       \n",
    "    2. We assume that the host system generates a finite number of system calls. \n",
    "    \n",
    "    3. We index each system call by using an integer starting from 1 and denote the fixed set of all possible system calls in the system as S = {1, · · · , K}. Let x = x1x2 · · · xl(xi ∈ S) denote a sequence of l system calls.\n",
    "       \n",
    "**LSTM Based Model :**     \n",
    "\n",
    "    1. At the Input Layer, the call at each time step xi is fed into the model in the form of one-hot encoding,\n",
    "       in other words, a K dimensional vector with all elements zero except position xi.\n",
    "       \n",
    "    2. At the Embedding Layer*, incoming calls are embedded to continuous space by multiplying embedding matrix W,\n",
    "       which should be learned. \n",
    "       \n",
    "    3. At the Hidden Layer*, the LSTM unit has an internal state, and this state is updated recurrently at each time step.\n",
    "    \n",
    "    4. At the Output Layer, a softmax activation function is used to produce the estimation of normalized probability values of possible calls coming next in the sequence.\n",
    "    \n",
    "**References for systemcalls:**\n",
    "    1. http://osinside.net/syscall/system_call_table.htm\n",
    "    2. https://www.cs.unm.edu/~immsec/systemcalls.htm    \n",
    "    3. https://github.com/karpathy/char-rnn\n",
    "    4. https://keras.io/losses/#categorical_crossentropy\n",
    "    5. http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADFA Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  1 13:52:35 2019\n",
    "\n",
    "@author: kuna\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# ignore all user warnings\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "def saveintopickle(obj, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print (\"[Pickle]: save object into {}\".format(filename))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def loadfrompickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "\n",
    "\n",
    "#draw the  process bar\n",
    "def drawProgressBar(percent, barLen = 20):\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        else:\n",
    "            progress += \" \"\n",
    "    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import io_helper\n",
    "\n",
    "\n",
    "random_data_dup = 10  # each sample randomly duplicated between 0 and 9 times, see dropin function\n",
    "\n",
    "\n",
    "def dropin(X, y):\n",
    "    \"\"\"\n",
    "    The name suggests the inverse of dropout, i.e. adding more samples. See Data Augmentation section at\n",
    "    http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/\n",
    "    :param X: Each row is a training sequence\n",
    "    :param y: Tne target we train and will later predict\n",
    "    :return: new augmented X, y\n",
    "    \"\"\"\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    X_hat = []\n",
    "    y_hat = []\n",
    "    for i in range(0, len(X)):\n",
    "        for j in range(0, np.random.random_integers(0, random_data_dup)):\n",
    "            X_hat.append(X[i, :])\n",
    "            y_hat.append(y[i])\n",
    "    return np.asarray(X_hat), np.asarray(y_hat)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    arrayfile = \"./array_test.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_train = array[:,:-1]\n",
    "    y_train = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_train.shape)\n",
    "    print (y_train.shape)\n",
    "    return (x_train,y_train)\n",
    "\n",
    "def preprocess_val():\n",
    "\n",
    "    arrayfile = \"./array_val.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_test = array[:,:-1]\n",
    "    y_test = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_test.shape)\n",
    "    print (y_test.shape)\n",
    "    return (x_test,y_test)\n",
    "\n",
    "#if __name__ ==\"__main__\":\n",
    "#   preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834\n",
      "Skip the file ADFA-LD/Training_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 63, 64, 65, 66, 75, 77, 78, 83, 85, 91, 93, 94, 96, 97, 99, 102, 104, 110, 114, 117, 118, 119, 120, 122, 125, 128, 132, 133, 140, 141, 142, 143, 144, 146, 148, 155, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 179, 180, 183, 184, 185, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 219, 220, 221, 224, 226, 228, 229, 230, 231, 233, 234, 240, 242, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 298, 300, 301, 307, 308, 309, 311, 314, 320, 322, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "833\n",
      "The maximum length of a sequence is that 2948\n",
      "lists_of_list_into_big_matrix\n",
      "833\n",
      "[ =                    ] 8.52%(20298, 20, 341)\n",
      "done\n",
      "[Pickle]: save object into array_test.pickle\n",
      "4373\n",
      "Skip the file ADFA-LD/Validation_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 22, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 61, 63, 64, 65, 66, 75, 77, 78, 79, 83, 85, 90, 91, 93, 94, 96, 97, 99, 102, 104, 110, 111, 114, 116, 117, 118, 119, 120, 122, 124, 125, 128, 132, 133, 136, 140, 141, 142, 143, 144, 146, 148, 150, 151, 154, 155, 156, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 190, 191, 192, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 224, 226, 228, 229, 231, 234, 240, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 296, 298, 300, 301, 306, 307, 308, 309, 311, 314, 320, 324, 328, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "4372\n",
      "The maximum length of a sequence is that 4494\n",
      "[                      ] 1.26%(21238, 20, 341)\n",
      "done\n",
      "[Pickle]: save object into array_val.pickle\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "#import io_helper\n",
    "\n",
    "def readfilesfromAdir(dataset):\n",
    "    #read a list of files\n",
    "    files = os.listdir(dataset)\n",
    "    files_absolute_paths = []\n",
    "    for i in files:\n",
    "        files_absolute_paths.append(dataset+str(i))\n",
    "    return files_absolute_paths\n",
    "\n",
    "\n",
    "file = \"ADFA-LD/Training_Data_Master/UTD-0001.txt\"\n",
    "#this is used to read a char sequence from\n",
    "def readCharsFromFile(file):\n",
    "    channel_values = open(file).read().split()\n",
    "    #print (len(channel_values))\n",
    "    #channel_values is a list\n",
    "    return channel_values\n",
    "    #print (channel_values[800:819])\n",
    "\n",
    "def get_attack_subdir(path):\n",
    "    subdirectories = os.listdir(path)\n",
    "    for i in range(0,len(subdirectories)):\n",
    "        subdirectories[i] = path + subdirectories[i]\n",
    "\n",
    "    print (subdirectories)\n",
    "    return (subdirectories)\n",
    "\n",
    "\n",
    "def get_all_call_sequences(dire):\n",
    "    files = readfilesfromAdir(dire)\n",
    "    allthelist = []\n",
    "    print (len(files))\n",
    "\n",
    "    for eachfile in files:\n",
    "        if not eachfile.endswith(\"DS_Store\"):\n",
    "            allthelist.append(readCharsFromFile(eachfile))\n",
    "        else:\n",
    "            print (\"Skip the file \"+ str(eachfile))\n",
    "\n",
    "    elements = []\n",
    "    for item in allthelist:\n",
    "        for key in item:\n",
    "            if key not in elements:\n",
    "                elements.append(key)\n",
    "\n",
    "    elements = map(int,elements)\n",
    "    elements = sorted(elements)\n",
    "\n",
    "    print (\"The total unique elements:\")\n",
    "    print (elements)\n",
    "\n",
    "    print (\"The maximum number of elements:\")\n",
    "    print (max(elements))\n",
    "\n",
    "    #print (\"The length elements:\")\n",
    "    #print (len(elements))\n",
    "    print (len(allthelist))\n",
    "\n",
    "    #clean the all list data set\n",
    "    _max = 0\n",
    "    for i in range(0,len(allthelist)):\n",
    "        _max = max(_max,len(allthelist[i]))\n",
    "        allthelist[i] = list(map(int,allthelist[i]))\n",
    "        #print(allthelist[i])\n",
    "\n",
    "\n",
    "    print (\"The maximum length of a sequence is that {}\".format(_max))\n",
    "\n",
    "    return (allthelist)\n",
    "\n",
    "## shift the data for analysis\n",
    "def shift(seq, n):\n",
    "    n = n % len(seq)\n",
    "    return seq[n:] + seq[:n]\n",
    "\n",
    "\n",
    "def convertToOneHot(vector, num_classes=None):\n",
    "    \"\"\"\n",
    "    Converts an input 1-D vector of integers into an output\n",
    "    2-D array of one-hot vectors, where an i'th input value\n",
    "    of j will set a '1' in the i'th row, j'th column of the\n",
    "    output array.\n",
    "\n",
    "    Example:\n",
    "        v = np.array((1, 0, 4))\n",
    "        one_hot_v = convertToOneHot(v)\n",
    "        print one_hot_v\n",
    "\n",
    "        [[0 1 0 0 0]\n",
    "         [1 0 0 0 0]\n",
    "         [0 0 0 0 1]]\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(vector, np.ndarray)\n",
    "    assert len(vector) > 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(vector)+1\n",
    "    else:\n",
    "        assert num_classes > 0\n",
    "        assert num_classes >= np.max(vector)\n",
    "\n",
    "    result = np.zeros(shape=(len(vector), num_classes))\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result.astype(int)\n",
    "\n",
    "\"\"\"\n",
    "The num_class here is set as 341\n",
    "\"\"\"\n",
    "\n",
    "#one function do one thing\n",
    "def sequence_n_gram_parsing(alist,n_gram=20,num_class=341):\n",
    "    if len(alist) <= n_gram:\n",
    "        return alist\n",
    "\n",
    "    ans = []\n",
    "    for i in range(0,len(alist)-n_gram+1,1):\n",
    "        tmp = alist[i:i+n_gram]\n",
    "        oneHot = convertToOneHot(np.asarray(tmp), num_class)\n",
    "        #print(tmp)\n",
    "        #print(np.asarray(tmp))\n",
    "        #print(oneHot)\n",
    "        ans.append(oneHot)\n",
    "\n",
    "    #transform into nmup arrray\n",
    "    ans = np.array(ans)\n",
    "    return (ans)\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix(allthelist,n_gram=20):\n",
    "    \n",
    "    print(\"lists_of_list_into_big_matrix\")\n",
    "    print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "    #print(len(allthelist[0]))\n",
    "    #print(allthelist[0])\n",
    "    #print(len(array))\n",
    "    #print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "       \n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "       \n",
    "        #print (\"tmp shape\")\n",
    "        #print(tmp)\n",
    "        #print (len(tmp))\n",
    " \n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "        #print(allthelist[i])\n",
    "        #print(array)\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "        #print(len(allthelist[1]))\n",
    "        #print(allthelist[1])\n",
    "        #print(len(array))\n",
    "        #print(array)\n",
    "        #break\n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_test.pickle\")\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix_val(allthelist,n_gram=20):\n",
    "\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_val.pickle\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dirc = \"ADFA-LD/Training_Data_Master/\"\n",
    "    dirc_val = \"ADFA-LD/Validation_Data_Master/\"\n",
    "    dic_attack =\"ADFA-LD/Attack_Data_Master/\"\n",
    "    #train1 = get_all_call_sequences(dirc)\n",
    "\n",
    "    #test = [i for i in range(0,300)]\n",
    "    #array = sequence_n_gram_parsing(test)\n",
    "    #print (type(array))\n",
    "    #print (array.shape)\n",
    "\n",
    "    #get_attack_subdir(dic_attack)\n",
    "    #print (\"XxxxxxxXXXXXXXXXXX\")\n",
    "    #val1 = get_all_call_sequences(dirc_val)\n",
    "    \n",
    "    #dirc_test = \"Test/\"\n",
    "    #att_test = get_all_call_sequences(dirc_test)\n",
    "    #lists_of_list_into_big_matrix(att_test)\n",
    "    \n",
    "    att = get_all_call_sequences(dirc)\n",
    "    lists_of_list_into_big_matrix(att)\n",
    "    \n",
    "    att_val = get_all_call_sequences(dirc_val)\n",
    "    lists_of_list_into_big_matrix_val(att_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "#import preprocess\n",
    "\n",
    "# Global hyper-parameters\n",
    "sequence_length = 19\n",
    "epochs = 1\n",
    "batch_size = 50\n",
    "feature_dimension = 341\n",
    "top_words = 5000\n",
    "\n",
    "def save_model_weight_into_file(model, modelname=\"model.json\", weight=\"model.h5\"):\n",
    "    model_json = model.to_json()\n",
    "    with open(modelname, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(weight)\n",
    "    print(\"Saved model to disk in {} and {}\".format(modelname,weight))\n",
    "\n",
    "\n",
    "def load_model_and_wieght_from_file(modelname=\"model.json\", weight=\"model.h5\"):\n",
    "\n",
    "    json_file = open(modelname, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weight)\n",
    "    print(\"Loaded model from disk, you can do more analysis more\")\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    layers = {'input': feature_dimension, 'hidden1': 64, 'hidden2': 256, 'hidden3': 100, 'output': feature_dimension}\n",
    "\n",
    "    model.add(LSTM(\n",
    "            input_length=sequence_length,\n",
    "            input_dim=layers['input'],\n",
    "            output_dim=layers['hidden1'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden2'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden3'],\n",
    "            return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "            output_dim=layers['output'],activation='softmax'))\n",
    "    #model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop',  metrics=['accuracy'])\n",
    "    #model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def run_network(model=None, data=None):\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    \n",
    "    if data is None:\n",
    "        print ('Loading data... ')\n",
    "        # train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "        X_train, y_train  = preprocess()\n",
    "    else:\n",
    "        X_train, y_train = data\n",
    "\n",
    "    print (\"X_train, y_train,shape\")\n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "    if model is None:\n",
    "        model = build_model()\n",
    "        #model = build_model_2()\n",
    "        print(\"Training...\")\n",
    "        model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_split=0.3)\n",
    "        model.summary()\n",
    "        print(\"Done Training...\")\n",
    "\n",
    "    #predicted = model.predict(X_test)\n",
    "    #print(\"Reshaping predicted\")\n",
    "    #predicted = np.reshape(predicted, (predicted.size,))\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"prediction exception\")\n",
    "        print 'Training duration (s) : ', time.time() - global_start_time\n",
    "        return model, y_test, 0\n",
    "   \n",
    "    try:\n",
    "        plt.figure(1)\n",
    "        plt.subplot(311)\n",
    "        plt.title(\"Actual Test Signal w/Anomalies\")\n",
    "        plt.plot(y_test[:len(y_test)], 'b')\n",
    "        plt.subplot(312)\n",
    "        plt.title(\"Predicted Signal\")\n",
    "        plt.plot(predicted[:len(y_test)], 'g')\n",
    "        plt.subplot(313)\n",
    "        plt.title(\"Squared Error\")\n",
    "        mse = ((y_test - predicted) ** 2)\n",
    "        plt.plot(mse, 'r')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"plotting exception\")\n",
    "        print (str(e))\n",
    "    print ('Training duration (s) : '% (time.time() - global_start_time))\n",
    "\n",
    "    return model, y_test, predicted\n",
    "   \"\"\"\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "# run_network()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Training...\n",
      "Train on 14208 samples, validate on 6090 samples\n",
      "Epoch 1/1\n",
      "14208/14208 [==============================] - 32s 2ms/step - loss: 2.7681 - acc: 0.2387 - val_loss: 2.8872 - val_acc: 0.1819\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 19, 64)            103936    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 19, 256)           328704    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 19, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 100)               142800    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 341)               34441     \n",
      "=================================================================\n",
      "Total params: 609,881\n",
      "Trainable params: 609,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "if model is None:\n",
    "    model = build_model()\n",
    "    print(\"Training...\")\n",
    "    history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.3,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    model.summary()\n",
    "    print(\"Done Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "#def loadData(file): \n",
    "    # for reading also binary mode is important \n",
    "#    dbfile = open(file, 'rb')      \n",
    "#    db = pickle.load(dbfile) \n",
    "#    for keys in db: \n",
    "#        print(keys, '=>', db[keys]) \n",
    "#    dbfile.close() \n",
    "  \n",
    "#if __name__ == '__main__': \n",
    "#    loadData(\"./array_test.pickle\") \n",
    "#df_val = pd.read_pickle(\"./array_val.pickle\")\n",
    "#df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data size is that \n",
      "(21238, 19, 341)\n",
      "(21238, 341)\n",
      "X_test, y_test,shape\n",
      "(21238, 19, 341)\n",
      "(21238, 341)\n",
      "Validating...\n",
      "Done Validating...\n",
      "[[1.2335530e-06 1.0198790e-03 8.9691122e-07 ... 1.0258395e-06\n",
      "  1.3941190e-06 3.1251540e-05]\n",
      " [1.0490133e-06 1.0960293e-03 7.4805553e-07 ... 8.8167792e-07\n",
      "  1.1079958e-06 2.8315437e-05]\n",
      " [1.5564030e-06 1.4356853e-03 1.0348892e-06 ... 1.2527308e-06\n",
      "  1.4993083e-06 3.9803337e-05]\n",
      " ...\n",
      " [4.5564093e-06 1.5376091e-02 2.3351497e-06 ... 3.3488861e-06\n",
      "  4.1597473e-06 1.1217311e-04]\n",
      " [3.9790325e-06 1.5034483e-02 2.0294226e-06 ... 2.9392420e-06\n",
      "  3.6070917e-06 1.0151930e-04]\n",
      " [3.7426239e-06 1.4799395e-02 1.9216072e-06 ... 2.7753281e-06\n",
      "  3.4382588e-06 9.7567863e-05]]\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "X_test, y_test = preprocess_val()\n",
    "\n",
    "print (\"X_test, y_test,shape\")\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "print(\"Validating...\")\n",
    "predicted = model.predict(X_test)\n",
    "print(\"Done Validating...\")\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did our model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 3.01\n",
      "Validation Accuracy : 0.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Score : %.2f'%(score))\n",
    "print('Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Loss')\n",
    "#plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [2.9082302657645718],\n",
       " 'val_acc': [0.19474548491693677],\n",
       " 'loss': [2.8591575310943096],\n",
       " 'acc': [0.21107826600331595]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Accuracy')\n",
    "#plt.plot(history.history['acc'], label='train')\n",
    "#plt.plot(history.history['val_acc'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Test with new systemcall  sequence ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hidden nodes is 4000.\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "\n",
    "word_vec_length = 19\n",
    "char_vec_length = 341\n",
    "output_labels = 341\n",
    "\n",
    "\n",
    "hidden_nodes = 4000 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "def build_model_2():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 2387s 118ms/step - loss: 3.0639 - acc: 0.2968 - val_loss: 2.7892 - val_acc: 0.4133\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 2296s 113ms/step - loss: 2.1557 - acc: 0.5167 - val_loss: 2.3318 - val_acc: 0.4857\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 2240s 110ms/step - loss: 1.5623 - acc: 0.6057 - val_loss: 2.3203 - val_acc: 0.5069\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 2263s 112ms/step - loss: 1.3943 - acc: 0.6450 - val_loss: 2.1959 - val_acc: 0.5048\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 2293s 113ms/step - loss: 1.1344 - acc: 0.6758 - val_loss: 2.1536 - val_acc: 0.5035\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 2309s 114ms/step - loss: 1.0304 - acc: 0.6982 - val_loss: 2.1643 - val_acc: 0.5033\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 2341s 115ms/step - loss: 0.9806 - acc: 0.7183 - val_loss: 2.2458 - val_acc: 0.5089\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 2351s 116ms/step - loss: 0.8993 - acc: 0.7389 - val_loss: 2.2913 - val_acc: 0.5078\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 2289s 113ms/step - loss: 0.8227 - acc: 0.7563 - val_loss: 2.2352 - val_acc: 0.5294\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 2275s 112ms/step - loss: 0.7431 - acc: 0.7794 - val_loss: 2.3909 - val_acc: 0.5145\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 4000)              69472000  \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 341)               1364341   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 70,836,341\n",
      "Trainable params: 70,836,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_2()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 2.39\n",
      "Validation Accuracy : 0.51\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Score : %.2f'%(score))\n",
    "print('Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## k-fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train\n",
    "Y = y_train\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 10-fold cross validation test harness\n",
    "#kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#cvscores = []\n",
    "#for train, test in kfold.split(X, Y):\n",
    "#  # create model\n",
    "#\tmodel = Sequential()\n",
    "#\tmodel.add(Dense(12, input_dim=341, activation='relu'))\n",
    "#\tmodel.add(Dense(8, activation='relu'))\n",
    "#\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "#\t# Compile model\n",
    "#\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#\t# Fit the model\n",
    "#\tmodel.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "#\t# evaluate the model\n",
    "#\tscores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "#\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#\tcvscores.append(scores[1] * 100)\n",
    "#print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hidden nodes is 100.\n",
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 28s 1ms/step - loss: 0.0113 - acc: 0.9971 - val_loss: 0.0097 - val_acc: 0.9973\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 20s 975us/step - loss: 0.0085 - acc: 0.9973 - val_loss: 0.0089 - val_acc: 0.9976\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 19s 953us/step - loss: 0.0075 - acc: 0.9975 - val_loss: 0.0083 - val_acc: 0.9977\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 20s 993us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0079 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 19s 935us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 19s 949us/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 20s 966us/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0079 - val_acc: 0.9978\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 19s 939us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 20s 965us/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 20s 979us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0080 - val_acc: 0.9977\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_26 (LSTM)               (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "\n",
    "word_vec_length = 19\n",
    "char_vec_length = 341\n",
    "output_labels = 341\n",
    "\n",
    "\n",
    "hidden_nodes = 100 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "def build_model_3():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_3()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 25s 1ms/step - loss: 0.0113 - acc: 0.9971 - val_loss: 0.0099 - val_acc: 0.9973\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 20s 972us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0087 - val_acc: 0.9977\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 20s 973us/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.0081 - val_acc: 0.9978\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 20s 964us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 19s 956us/step - loss: 0.0061 - acc: 0.9979 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 19s 940us/step - loss: 0.0058 - acc: 0.9980 - val_loss: 0.0077 - val_acc: 0.9978\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 20s 988us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 19s 960us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0079 - val_acc: 0.9978\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 19s 950us/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0078 - val_acc: 0.9978\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 19s 956us/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "def build_model_4():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_4()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 23s 1ms/step - loss: 0.0110 - acc: 0.9971 - val_loss: 0.0096 - val_acc: 0.9975\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 19s 945us/step - loss: 0.0079 - acc: 0.9975 - val_loss: 0.0085 - val_acc: 0.9977\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 19s 950us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0079 - val_acc: 0.9978\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 20s 963us/step - loss: 0.0062 - acc: 0.9979 - val_loss: 0.0081 - val_acc: 0.9978\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 19s 928us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0079 - val_acc: 0.9977\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 19s 920us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0079 - val_acc: 0.9978\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 18s 898us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0080 - val_acc: 0.9978\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 18s 909us/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0082 - val_acc: 0.9977\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 19s 926us/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0081 - val_acc: 0.9977\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 19s 928us/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0083 - val_acc: 0.9978\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "def build_model_5():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_5()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.01\n",
      "Validation Accuracy : 1.00\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Score : %.2f'%(score))\n",
    "print('Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834\n",
      "Skip the file ADFA-LD/Training_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 63, 64, 65, 66, 75, 77, 78, 83, 85, 91, 93, 94, 96, 97, 99, 102, 104, 110, 114, 117, 118, 119, 120, 122, 125, 128, 132, 133, 140, 141, 142, 143, 144, 146, 148, 155, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 179, 180, 183, 184, 185, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 219, 220, 221, 224, 226, 228, 229, 230, 231, 233, 234, 240, 242, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 298, 300, 301, 307, 308, 309, 311, 314, 320, 322, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "833\n",
      "The maximum length of a sequence is that 2948\n",
      "lists_of_list_into_big_matrix train\n",
      "833\n",
      "819\n",
      "[6, 6, 63, 6, 42, 120, 6, 195, 120, 6, 6, 114, 114, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 1, 252, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 1, 1, 252, 1, 252, 252, 252, 252, 252, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 252, 1, 1, 252, 1, 1, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252, 252, 1, 1, 1, 1, 1, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 1, 252, 252, 1, 1, 252, 252, 252, 1, 1, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, 1, 252, 1, 1, 252, 1, 1, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 1, 1, 252, 1, 252, 252, 252, 1, 252, 252, 252, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 252]\n",
      "810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ =                    ] 8.16%(20450, 10, 341)\n",
      "done\n",
      "[Pickle]: save object into array_test.pickle\n",
      "4373\n",
      "Skip the file ADFA-LD/Validation_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 22, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 61, 63, 64, 65, 66, 75, 77, 78, 79, 83, 85, 90, 91, 93, 94, 96, 97, 99, 102, 104, 110, 111, 114, 116, 117, 118, 119, 120, 122, 124, 125, 128, 132, 133, 136, 140, 141, 142, 143, 144, 146, 148, 150, 151, 154, 155, 156, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 190, 191, 192, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 224, 226, 228, 229, 231, 234, 240, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 296, 298, 300, 301, 306, 307, 308, 309, 311, 314, 320, 324, 328, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "4372\n",
      "The maximum length of a sequence is that 4494\n",
      "lists_of_list_into_big_matrix validation\n",
      "4372\n",
      "1950\n",
      "[6, 11, 45, 33, 192, 33, 5, 197, 192, 6, 33, 5, 3, 197, 192, 192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 3, 197, 192, 192, 192, 6, 33, 5, 3, 197, 192, 125, 192, 6, 33, 5, 3, 197, 192, 192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 3, 197, 192, 192, 6, 33, 5, 3, 197, 192, 192, 192, 192, 6, 192, 243, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 91, 258, 311, 240, 240, 174, 174, 175, 191, 122, 54, 45, 45, 192, 5, 197, 192, 3, 6, 91, 197, 197, 197, 197, 85, 85, 195, 195, 195, 195, 195, 195, 195, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 174, 195, 195, 195, 5, 5, 6, 6, 6, 195, 5, 5, 5, 197, 5, 197, 192, 3, 6, 91, 196, 196, 195, 5, 6, 3, 197, 6, 195, 197, 6, 91, 195, 192, 6, 6, 5, 197, 197, 192, 3, 3, 3, 3, 3, 3, 3, 3, 91, 45, 5, 197, 197, 192, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 91, 195, 195, 195, 195, 5, 221, 220, 220, 5, 220, 5, 6, 3, 5, 195, 5, 195, 3, 195, 5, 5, 195, 195, 3, 6, 5, 5, 195, 195, 6, 6, 91, 6, 6, 195, 197, 6, 54, 3, 3, 192, 3, 3, 192, 3, 3, 91, 6, 91, 45, 195, 195, 195, 5, 5, 5, 5, 195, 5, 5, 3, 3, 6, 91, 195, 91, 195, 3, 195, 5, 5, 125, 6, 6, 195, 5, 6, 6, 195, 5, 5, 5, 5, 195, 5, 5, 5, 3, 6, 195, 45, 3, 6, 91, 195, 5, 195, 6, 6, 6, 195, 5, 5, 6, 195, 5, 5, 195, 5, 195, 5, 5, 195, 6, 6, 6, 195, 195, 195, 6, 195, 5, 3, 45, 6, 91, 195, 5, 5, 6, 191, 6, 195, 5, 5, 6, 6, 195, 5, 5, 5, 3, 33, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 196, 195, 85, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 197, 192, 3, 3, 3, 6, 91, 5, 197, 196, 196, 196, 195, 195, 196, 196, 195, 196, 195, 196, 196, 196, 196, 5, 220, 220, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 196, 195, 196, 195, 196, 195, 196, 195, 196, 195, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 195, 196, 195, 196, 196, 196, 195, 195, 195, 196, 85, 40, 220, 220, 195, 195, 195, 195, 195, 195, 196, 196, 195, 196, 195, 196, 196, 196, 196, 196, 195, 196, 196, 196, 196, 195, 196, 40, 5, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 195, 196, 220, 220, 6, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 195, 195, 195, 195, 195, 195, 196, 196, 196, 195, 196, 195, 196, 196, 195, 195, 195, 196, 40, 220, 220, 6, 195, 195, 196, 195, 195, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 196, 196, 195, 196, 195, 196, 195, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 195, 196, 195, 196, 196, 196, 196, 5, 220, 220, 195, 195, 195, 195, 195, 196, 196, 195, 195, 195, 195, 196, 195, 196, 195, 196, 195, 220, 220, 6, 195, 195, 195, 195, 195, 196, 195, 195, 195, 195, 196, 196, 40, 220, 220, 195, 195, 220, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 196, 196, 40, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 195, 196, 196, 196, 196, 220, 195, 195, 195, 196, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 196, 195, 196, 196, 40, 220, 220, 6, 195, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 196, 5, 220, 195, 195, 195, 195, 195, 195, 196, 85, 195, 196, 196, 196, 196, 5, 220, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 85, 195, 196, 196, 196, 195, 195, 195, 195, 196, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 196, 196, 195, 195, 196, 85, 195, 5, 220, 220, 195, 195, 195, 195, 195, 195, 195, 85, 196, 196, 196, 196, 196, 196, 220, 195, 196, 196, 196, 195, 196, 196, 196, 196, 196, 195, 196, 196, 195, 196, 196, 196, 220, 195, 195, 195, 195, 195, 196, 85, 85, 196, 196, 196, 196, 196, 195, 196, 196, 196, 196, 196, 196, 220, 195, 220, 220, 6, 195, 195, 195, 196, 195, 85, 196, 195, 196, 40, 196, 5, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 40, 196, 5, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 40, 196, 5, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 196, 195, 85, 196, 195, 40, 196, 5, 220, 220, 6, 195, 195, 195, 196, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 220, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 196, 40, 220, 220, 6, 195, 195, 195, 195, 195, 85, 196, 195, 196, 196, 196, 220, 220, 6, 195, 195, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 195, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 196, 196, 195, 220, 195, 85, 196, 196, 196, 5, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 195, 195, 85, 196, 196, 195, 196, 220, 220, 6, 195, 195, 195, 195, 196, 195, 85, 196, 5, 220, 220, 6, 195, 195, 195, 195, 196, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 85, 220, 220, 6, 195, 195, 195, 195, 195, 195, 196, 196, 85, 196, 85, 196, 196, 196, 220, 220, 85, 196, 85, 196, 220, 220, 6, 195, 195, 196, 196, 196, 196, 196, 220, 220, 195, 195, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 85, 195, 195, 195, 195, 195, 196, 196, 196, 195, 196, 196, 195, 195, 85, 85, 196, 220, 195, 195, 195, 196, 195, 196, 196, 220, 220, 6, 195, 220, 195, 195, 195, 196, 196, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 196, 85, 195, 195, 195, 196, 196, 196, 195, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 195, 196, 196, 5, 220, 220, 6, 195, 195, 196, 196, 195, 196, 220, 220, 6, 195, 195, 195, 196, 85, 196, 195, 196, 196, 195, 220, 220, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 220, 196, 220, 220, 6, 195, 195, 195, 195, 220, 195, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 220, 220, 195, 195, 195, 195, 196, 195, 195, 195, 195, 195, 195, 196, 196, 196, 40, 196, 196, 220, 220, 6, 195, 195, 196, 196, 196, 220, 220, 196, 220, 195, 195, 196, 195, 195, 195, 196, 196, 195, 196, 196, 220, 220, 6, 195, 195, 195, 196, 195, 196, 220, 220, 6, 195, 195, 195, 195, 195, 195, 195, 196, 196, 195, 196, 195, 196, 196, 220, 220, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 195, 195, 196, 196, 196, 195, 196, 196, 195, 196, 196, 196, 85, 196, 196, 195, 195, 195, 195, 220, 195, 195, 85, 40, 220, 220, 195, 195, 195, 196, 220, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 195, 196, 196, 196, 195, 196, 196, 196, 220, 220, 195, 195, 195, 195, 196, 196, 196, 196, 196, 196, 40, 220, 220, 195, 196, 196, 220, 220, 195, 195, 196, 220, 85, 196, 196, 5, 220, 220, 195, 3, 195, 3, 3, 5, 3, 3, 5, 6, 195, 195, 195, 195, 195, 195, 195, 195, 195]\n",
      "1941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                      ] 1.21%(20157, 10, 341)\n",
      "done\n",
      "[Pickle]: save object into array_val.pickle\n"
     ]
    }
   ],
   "source": [
    "def preprocess():\n",
    "\n",
    "    arrayfile = \"./array_test.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_train = array[:,:-1]\n",
    "    y_train = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_train.shape)\n",
    "    print (y_train.shape)\n",
    "    return (x_train,y_train)\n",
    "\n",
    "def preprocess_val():\n",
    "\n",
    "    arrayfile = \"./array_val.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_test = array[:,:-1]\n",
    "    y_test = array[:,-1]\n",
    "\n",
    "    print (\"The validation data size is that \")\n",
    "    print (x_test.shape)\n",
    "    print (y_test.shape)\n",
    "    return (x_test,y_test)\n",
    "\n",
    "def preprocess_attack():\n",
    "\n",
    "    arrayfile = \"./array_attack.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_attack = array[:,:-1]\n",
    "    y_attack = array[:,-1]\n",
    "\n",
    "    print (\"The attack data size is that \")\n",
    "    print (x_attack.shape)\n",
    "    print (y_attack.shape)\n",
    "    return (x_attack,y_test)\n",
    "\n",
    "\n",
    "\n",
    "def convertToOneHot(vector, num_classes=None):\n",
    "    \"\"\"\n",
    "    Converts an input 1-D vector of integers into an output\n",
    "    2-D array of one-hot vectors, where an i'th input value\n",
    "    of j will set a '1' in the i'th row, j'th column of the\n",
    "    output array.\n",
    "\n",
    "    Example:\n",
    "        v = np.array((1, 0, 4))\n",
    "        one_hot_v = convertToOneHot(v)\n",
    "        print one_hot_v\n",
    "\n",
    "        [[0 1 0 0 0]\n",
    "         [1 0 0 0 0]\n",
    "         [0 0 0 0 1]]\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(vector, np.ndarray)\n",
    "    assert len(vector) > 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(vector)+1\n",
    "    else:\n",
    "        assert num_classes > 0\n",
    "        assert num_classes >= np.max(vector)\n",
    "\n",
    "    result = np.zeros(shape=(len(vector), num_classes))\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result.astype(int)\n",
    "\n",
    "\"\"\"\n",
    "The num_class here is set as 341\n",
    "\"\"\"\n",
    "\n",
    "#one function do one thing\n",
    "def sequence_n_gram_parsing(alist,n_gram=10,num_class=341):\n",
    "    if len(alist) <= n_gram:\n",
    "        return alist\n",
    "\n",
    "    ans = []\n",
    "    for i in range(0,len(alist)-n_gram+1,1):\n",
    "        tmp = alist[i:i+n_gram]\n",
    "        oneHot = convertToOneHot(np.asarray(tmp), num_class)\n",
    "        #print(tmp)\n",
    "        #print(np.asarray(tmp))\n",
    "        #print(oneHot)\n",
    "        ans.append(oneHot)\n",
    "\n",
    "    #transform into nmup arrray\n",
    "    ans = np.array(ans)\n",
    "    return (ans)\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix(allthelist,n_gram=10):\n",
    "    \n",
    "    print(\"lists_of_list_into_big_matrix train\")\n",
    "    print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "    print(len(allthelist[0]))\n",
    "    print(allthelist[0])\n",
    "    print(len(array))\n",
    "    print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "       \n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "       \n",
    "        #print (\"tmp shape\")\n",
    "        #print(tmp)\n",
    "        #print (len(tmp))\n",
    " \n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "        #print(allthelist[i])\n",
    "        #print(array)\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "        #print(len(allthelist[1]))\n",
    "        #print(allthelist[1])\n",
    "        #print(len(array))\n",
    "        #print(array)\n",
    "        #break\n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_test.pickle\")\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix_val(allthelist,n_gram=10):\n",
    "\n",
    "    print(\"lists_of_list_into_big_matrix validation\")\n",
    "    print(len(allthelist))\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "    print(len(allthelist[0]))\n",
    "    print(allthelist[0])\n",
    "    print(len(array))\n",
    "    print(array)\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_val.pickle\")\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix_attack(allthelist,n_gram=10):\n",
    "\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "       \n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_attack.pickle\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    dirc = \"ADFA-LD/Training_Data_Master/\"\n",
    "    dirc_val = \"ADFA-LD/Validation_Data_Master/\"\n",
    "    dic_attack =\"ADFA-LD/Attack_Data_Master_All/\"\n",
    "    #train1 = get_all_call_sequences(dirc)\n",
    "\n",
    "    #test = [i for i in range(0,300)]\n",
    "    #array = sequence_n_gram_parsing(test)\n",
    "    #print (type(array))\n",
    "    #print (array.shape)\n",
    "\n",
    "    #get_attack_subdir(dic_attack)\n",
    "    #print (\"XxxxxxxXXXXXXXXXXX\")\n",
    "    #val1 = get_all_call_sequences(dirc_val)\n",
    "    \n",
    "    #dirc_test = \"Test/\"\n",
    "    #att_test = get_all_call_sequences(dirc_test)\n",
    "    #lists_of_list_into_big_matrix(att_test)\n",
    "    \n",
    "    att = get_all_call_sequences(dirc)\n",
    "    lists_of_list_into_big_matrix(att)\n",
    "    \n",
    "    att_val = get_all_call_sequences(dirc_val)\n",
    "    lists_of_list_into_big_matrix_val(att_val)\n",
    "    \n",
    "    #att_attack = get_all_call_sequences(dic_attack)\n",
    "    #lists_of_list_into_big_matrix_attack(att_attack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20450, 9, 341)\n",
      "(20450, 341)\n",
      "X_train, y_train,shape\n",
      "(20450, 9, 341)\n",
      "(20450, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 14315 samples, validate on 6135 samples\n",
      "Epoch 1/10\n",
      "14315/14315 [==============================] - 12s 821us/step - loss: 0.0108 - acc: 0.9972 - val_loss: 0.0100 - val_acc: 0.9972\n",
      "Epoch 2/10\n",
      "14315/14315 [==============================] - 7s 481us/step - loss: 0.0078 - acc: 0.9975 - val_loss: 0.0090 - val_acc: 0.9974\n",
      "Epoch 3/10\n",
      "14315/14315 [==============================] - 8s 535us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0086 - val_acc: 0.9974\n",
      "Epoch 4/10\n",
      "14315/14315 [==============================] - 7s 498us/step - loss: 0.0064 - acc: 0.9978 - val_loss: 0.0081 - val_acc: 0.9976\n",
      "Epoch 5/10\n",
      "14315/14315 [==============================] - 7s 501us/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0079 - val_acc: 0.9976\n",
      "Epoch 6/10\n",
      "14315/14315 [==============================] - 7s 494us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.0076 - val_acc: 0.9977\n",
      "Epoch 7/10\n",
      "14315/14315 [==============================] - 7s 493us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0074 - val_acc: 0.9977\n",
      "Epoch 8/10\n",
      "14315/14315 [==============================] - 7s 466us/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0075 - val_acc: 0.9977\n",
      "Epoch 9/10\n",
      "14315/14315 [==============================] - 7s 484us/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0074 - val_acc: 0.9977\n",
      "Epoch 10/10\n",
      "14315/14315 [==============================] - 7s 486us/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0074 - val_acc: 0.9977\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 100)               176800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 341)               34441     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 211,241\n",
      "Trainable params: 211,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "word_vec_length = 9\n",
    "char_vec_length = 341\n",
    "output_labels = 341\n",
    "hidden_nodes = 100 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "def build_model_6():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_6()\n",
    "print(\"Training...\")\n",
    "history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.3,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "model.summary()\n",
    "print(\"Done Training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation data size is that \n",
      "(20157, 9, 341)\n",
      "(20157, 341)\n",
      "X_test, y_test,shape\n",
      "(20157, 9, 341)\n",
      "(20157, 341)\n",
      "Validating...\n",
      "Done Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 0.01\n",
      "Validation Accuracy : 1.00\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = preprocess_val()\n",
    "\n",
    "print (\"X_test, y_test,shape\")\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "\n",
    "print(\"Validating...\")\n",
    "predicted = model.predict(X_test)\n",
    "print(\"Done Validating...\")\n",
    "print(predicted)\n",
    "\n",
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Score : %.2f'%(score))\n",
    "print('Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input/Output Data to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "#https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras/\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=numpy.nan)\n",
    "\n",
    "def int_to_onehot(n, n_classes):\n",
    "    v = [0] * n_classes\n",
    "    v[n] = 1\n",
    "    return v\n",
    "\n",
    "def onehot_to_int(v):\n",
    "    return v.index(1)\n",
    "\n",
    "X_train, y_train, X_test, y_test\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(X_train[:1,:,:])\n",
    "\n",
    "# systemcall trace-1 length = 819, \n",
    "# [6, 6, 63, 6, 42, 120, 6, 195, 120, 6, 6, 114, 114, 1, 1, 252, 252,\n",
    "# 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252,\n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 1, 252, 1, 1, 1,\n",
    "# 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 1, 1, 1, 1,\n",
    "# 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 1, 1, 252, 1, 252, 252, 252, \n",
    "# 252, 252, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 252, 1, 1, 252, 1, 1, 252, 1, 1, 252, 252, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 1, 1, 1, \n",
    "# 252, 1, 1, 1, 1, 1, 1, 252, 252, 1, 1, 1, 1, 1, 252, 252, 252, 252, 1, \n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
    "# 252, 252, 252, 252, 252, 1, 252, 252, 1, 252, 252, 1, 1, 252, 252, 252, \n",
    "# 1, 1, 252, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 252, 252, 252, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1, 252, 252, 1, \n",
    "# 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 1,\n",
    "# 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 252, 1, 1, 1, 1, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 252, 1, 1, 252, 1, 1, 252, 1, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 252, 252, 1, 252, 1, 1, 252, 1, 252, 252, 252, 1, 252, \n",
    "# 252, 252, 1, 1, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, \n",
    "# 252, 252, 252, 1, 1, 252]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Sequence [6, 6, 63, 6, 42, 120, 6, 195, 120, 6]\n",
    "# [X -> 6, 6, 63, 6, 42, 120, 6, 195, 120, Y-> 6]\n",
    "pprint.pprint(y_train[:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sequence [114 ,162, 114, 114 ,162, 114, 162, 162]\n",
    "# [X ->114, 162 ,114, 114 ,162, 114, 162  Y-> 162]\n",
    "\n",
    "test_input = array([[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0]]])\n",
    "\n",
    "test_input = test_input.reshape((1, 19, 341))\n",
    "test_output = model.predict(test_input, verbose=0)\n",
    "print(test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
