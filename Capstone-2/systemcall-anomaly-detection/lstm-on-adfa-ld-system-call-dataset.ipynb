{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System call Anomaly Detection- Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADFA Dataset Preprocessing:**\n",
    "\n",
    "    1. The system call language model estimates the probability distribution of the next call in a sequence given the sequence of previous calls. \n",
    "       \n",
    "    2. We assume that the host system generates a finite number of system calls. \n",
    "    \n",
    "    3. We index each system call by using an integer starting from 1 and denote the fixed set of all possible system calls in the system as S = {1, · · · , K}. Let x = x1x2 · · · xl(xi ∈ S) denote a sequence of l system calls.\n",
    "       \n",
    "**LSTM Based Model :**     \n",
    "\n",
    "    1. At the Input Layer, the call at each time step xi is fed into the model in the form of one-hot encoding,\n",
    "       in other words, a K dimensional vector with all elements zero except position xi.\n",
    "       \n",
    "    2. At the Embedding Layer*, incoming calls are embedded to continuous space by multiplying embedding matrix W,\n",
    "       which should be learned. \n",
    "       \n",
    "    3. At the Hidden Layer*, the LSTM unit has an internal state, and this state is updated recurrently at each time step.\n",
    "    \n",
    "    4. At the Output Layer, a softmax activation function is used to produce the estimation of normalized probability values of possible calls coming next in the sequence.\n",
    "    \n",
    "**References for systemcalls:**\n",
    "    1. http://osinside.net/syscall/system_call_table.htm\n",
    "    2. https://www.cs.unm.edu/~immsec/systemcalls.htm    \n",
    "    3. https://github.com/karpathy/char-rnn\n",
    "    4. https://keras.io/losses/#categorical_crossentropy\n",
    "    5. http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADFA Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  1 13:52:35 2019\n",
    "\n",
    "@author: kuna\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# ignore all user warnings\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "def saveintopickle(obj, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print (\"[Pickle]: save object into {}\".format(filename))\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def loadfrompickle(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "\n",
    "\n",
    "#draw the  process bar\n",
    "def drawProgressBar(percent, barLen = 20):\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        else:\n",
    "            progress += \" \"\n",
    "    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import io_helper\n",
    "\n",
    "\n",
    "random_data_dup = 10  # each sample randomly duplicated between 0 and 9 times, see dropin function\n",
    "\n",
    "\n",
    "def dropin(X, y):\n",
    "    \"\"\"\n",
    "    The name suggests the inverse of dropout, i.e. adding more samples. See Data Augmentation section at\n",
    "    http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/\n",
    "    :param X: Each row is a training sequence\n",
    "    :param y: Tne target we train and will later predict\n",
    "    :return: new augmented X, y\n",
    "    \"\"\"\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    X_hat = []\n",
    "    y_hat = []\n",
    "    for i in range(0, len(X)):\n",
    "        for j in range(0, np.random.random_integers(0, random_data_dup)):\n",
    "            X_hat.append(X[i, :])\n",
    "            y_hat.append(y[i])\n",
    "    return np.asarray(X_hat), np.asarray(y_hat)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    arrayfile = \"./array_test.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_train = array[:,:-1]\n",
    "    y_train = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_train.shape)\n",
    "    print (y_train.shape)\n",
    "    return (x_train,y_train)\n",
    "\n",
    "def preprocess_val():\n",
    "\n",
    "    arrayfile = \"./array_val.pickle\"\n",
    "    array = loadfrompickle(arrayfile)\n",
    "    #print(type(array))\n",
    "    #print(array)\n",
    "    x_test = array[:,:-1]\n",
    "    y_test = array[:,-1]\n",
    "\n",
    "    print (\"The train data size is that \")\n",
    "    print (x_test.shape)\n",
    "    print (y_test.shape)\n",
    "    return (x_test,y_test)\n",
    "\n",
    "#if __name__ ==\"__main__\":\n",
    "#   preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834\n",
      "Skip the file ADFA-LD/Training_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 63, 64, 65, 66, 75, 77, 78, 83, 85, 91, 93, 94, 96, 97, 99, 102, 104, 110, 114, 117, 118, 119, 120, 122, 125, 128, 132, 133, 140, 141, 142, 143, 144, 146, 148, 155, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 179, 180, 183, 184, 185, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 211, 212, 213, 214, 219, 220, 221, 224, 226, 228, 229, 230, 231, 233, 234, 240, 242, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 298, 300, 301, 307, 308, 309, 311, 314, 320, 322, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "833\n",
      "The maximum length of a sequence is that 2948\n",
      "[ =                    ] 8.52%(20298, 20, 341)\n",
      "done\n",
      "[Pickle]: save object into array_test.pickle\n",
      "4373\n",
      "Skip the file ADFA-LD/Validation_Data_Master/.DS_Store\n",
      "The total unique elements:\n",
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 19, 20, 21, 22, 26, 27, 30, 33, 37, 38, 39, 40, 41, 42, 43, 45, 54, 57, 60, 61, 63, 64, 65, 66, 75, 77, 78, 79, 83, 85, 90, 91, 93, 94, 96, 97, 99, 102, 104, 110, 111, 114, 116, 117, 118, 119, 120, 122, 124, 125, 128, 132, 133, 136, 140, 141, 142, 143, 144, 146, 148, 150, 151, 154, 155, 156, 157, 158, 159, 160, 162, 163, 168, 172, 174, 175, 176, 177, 179, 180, 181, 183, 184, 185, 186, 187, 190, 191, 192, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 224, 226, 228, 229, 231, 234, 240, 243, 252, 254, 255, 256, 258, 259, 260, 264, 265, 266, 268, 269, 270, 272, 289, 292, 293, 295, 296, 298, 300, 301, 306, 307, 308, 309, 311, 314, 320, 324, 328, 331, 332, 340]\n",
      "The maximum number of elements:\n",
      "340\n",
      "4372\n",
      "The maximum length of a sequence is that 4494\n",
      "[                      ] 1.26%(21238, 20, 341)\n",
      "done\n",
      "[Pickle]: save object into array_val.pickle\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "#import io_helper\n",
    "\n",
    "def readfilesfromAdir(dataset):\n",
    "    #read a list of files\n",
    "    files = os.listdir(dataset)\n",
    "    files_absolute_paths = []\n",
    "    for i in files:\n",
    "        files_absolute_paths.append(dataset+str(i))\n",
    "    return files_absolute_paths\n",
    "\n",
    "\n",
    "file = \"ADFA-LD/Training_Data_Master/UTD-0001.txt\"\n",
    "#this is used to read a char sequence from\n",
    "def readCharsFromFile(file):\n",
    "    channel_values = open(file).read().split()\n",
    "    #print (len(channel_values))\n",
    "    #channel_values is a list\n",
    "    return channel_values\n",
    "    #print (channel_values[800:819])\n",
    "\n",
    "def get_attack_subdir(path):\n",
    "    subdirectories = os.listdir(path)\n",
    "    for i in range(0,len(subdirectories)):\n",
    "        subdirectories[i] = path + subdirectories[i]\n",
    "\n",
    "    print (subdirectories)\n",
    "    return (subdirectories)\n",
    "\n",
    "\n",
    "def get_all_call_sequences(dire):\n",
    "    files = readfilesfromAdir(dire)\n",
    "    allthelist = []\n",
    "    print (len(files))\n",
    "\n",
    "    for eachfile in files:\n",
    "        if not eachfile.endswith(\"DS_Store\"):\n",
    "            allthelist.append(readCharsFromFile(eachfile))\n",
    "        else:\n",
    "            print (\"Skip the file \"+ str(eachfile))\n",
    "\n",
    "    elements = []\n",
    "    for item in allthelist:\n",
    "        for key in item:\n",
    "            if key not in elements:\n",
    "                elements.append(key)\n",
    "\n",
    "    elements = map(int,elements)\n",
    "    elements = sorted(elements)\n",
    "\n",
    "    print (\"The total unique elements:\")\n",
    "    print (elements)\n",
    "\n",
    "    print (\"The maximum number of elements:\")\n",
    "    print (max(elements))\n",
    "\n",
    "    #print (\"The length elements:\")\n",
    "    #print (len(elements))\n",
    "    print (len(allthelist))\n",
    "\n",
    "    #clean the all list data set\n",
    "    _max = 0\n",
    "    for i in range(0,len(allthelist)):\n",
    "        _max = max(_max,len(allthelist[i]))\n",
    "        allthelist[i] = list(map(int,allthelist[i]))\n",
    "        #print(allthelist[i])\n",
    "\n",
    "\n",
    "    print (\"The maximum length of a sequence is that {}\".format(_max))\n",
    "\n",
    "    return (allthelist)\n",
    "\n",
    "## shift the data for analysis\n",
    "def shift(seq, n):\n",
    "    n = n % len(seq)\n",
    "    return seq[n:] + seq[:n]\n",
    "\n",
    "\n",
    "def convertToOneHot(vector, num_classes=None):\n",
    "    \"\"\"\n",
    "    Converts an input 1-D vector of integers into an output\n",
    "    2-D array of one-hot vectors, where an i'th input value\n",
    "    of j will set a '1' in the i'th row, j'th column of the\n",
    "    output array.\n",
    "\n",
    "    Example:\n",
    "        v = np.array((1, 0, 4))\n",
    "        one_hot_v = convertToOneHot(v)\n",
    "        print one_hot_v\n",
    "\n",
    "        [[0 1 0 0 0]\n",
    "         [1 0 0 0 0]\n",
    "         [0 0 0 0 1]]\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(vector, np.ndarray)\n",
    "    assert len(vector) > 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(vector)+1\n",
    "    else:\n",
    "        assert num_classes > 0\n",
    "        assert num_classes >= np.max(vector)\n",
    "\n",
    "    result = np.zeros(shape=(len(vector), num_classes))\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result.astype(int)\n",
    "\n",
    "\"\"\"\n",
    "The num_class here is set as 341\n",
    "\"\"\"\n",
    "\n",
    "#one function do one thing\n",
    "def sequence_n_gram_parsing(alist,n_gram=20,num_class=341):\n",
    "    if len(alist) <= n_gram:\n",
    "        return alist\n",
    "\n",
    "    ans = []\n",
    "    for i in range(0,len(alist)-n_gram+1,1):\n",
    "        tmp = alist[i:i+n_gram]\n",
    "        oneHot = convertToOneHot(np.asarray(tmp), num_class)\n",
    "        ans.append(oneHot)\n",
    "\n",
    "    #transform into nmup arrray\n",
    "    ans = np.array(ans)\n",
    "    return (ans)\n",
    "\n",
    "def lists_of_list_into_big_matrix(allthelist,n_gram=20):\n",
    "\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "\n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_test.pickle\")\n",
    "\n",
    "\n",
    "def lists_of_list_into_big_matrix_val(allthelist,n_gram=20):\n",
    "\n",
    "    array = sequence_n_gram_parsing(allthelist[0])\n",
    "\n",
    "    for i in range(1,len(allthelist),1):\n",
    "        tmp = sequence_n_gram_parsing(allthelist[i])\n",
    "\n",
    "       # print (\"tmp shape\")\n",
    "       # print (tmp.shape)\n",
    "\n",
    "        array = np.concatenate((array, tmp), axis=0)\n",
    "\n",
    "\n",
    "        percent = (i+0.0)/len(allthelist)\n",
    "        #io_helper.drawProgressBar(percent)\n",
    "        drawProgressBar(percent)\n",
    "\n",
    "        if (len(array)> 20000):\n",
    "            break\n",
    "        #print (\"array shape\")\n",
    "        #print (array.shape)\n",
    "\n",
    "\n",
    "    print (array.shape)\n",
    "    print (\"done\")\n",
    "    #io_helper.saveintopickle(array,\"array_test.pickle\")\n",
    "    saveintopickle(array,\"array_val.pickle\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dirc = \"ADFA-LD/Training_Data_Master/\"\n",
    "    dirc_val = \"ADFA-LD/Validation_Data_Master/\"\n",
    "    dic_attack =\"ADFA-LD/Attack_Data_Master/\"\n",
    "    #train1 = get_all_call_sequences(dirc)\n",
    "\n",
    "    #test = [i for i in range(0,300)]\n",
    "    #array = sequence_n_gram_parsing(test)\n",
    "    #print (type(array))\n",
    "    #print (array.shape)\n",
    "\n",
    "    #get_attack_subdir(dic_attack)\n",
    "    #print (\"XxxxxxxXXXXXXXXXXX\")\n",
    "    #val1 = get_all_call_sequences(dirc_val)\n",
    "    att = get_all_call_sequences(dirc)\n",
    "    lists_of_list_into_big_matrix(att)\n",
    "    att_val = get_all_call_sequences(dirc_val)\n",
    "    lists_of_list_into_big_matrix_val(att_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "#import preprocess\n",
    "\n",
    "# Global hyper-parameters\n",
    "sequence_length = 19\n",
    "epochs = 1\n",
    "batch_size = 50\n",
    "feature_dimension = 341\n",
    "top_words = 5000\n",
    "\n",
    "def save_model_weight_into_file(model, modelname=\"model.json\", weight=\"model.h5\"):\n",
    "    model_json = model.to_json()\n",
    "    with open(modelname, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(weight)\n",
    "    print(\"Saved model to disk in {} and {}\".format(modelname,weight))\n",
    "\n",
    "\n",
    "def load_model_and_wieght_from_file(modelname=\"model.json\", weight=\"model.h5\"):\n",
    "\n",
    "    json_file = open(modelname, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weight)\n",
    "    print(\"Loaded model from disk, you can do more analysis more\")\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    layers = {'input': feature_dimension, 'hidden1': 64, 'hidden2': 256, 'hidden3': 100, 'output': feature_dimension}\n",
    "\n",
    "    model.add(LSTM(\n",
    "            input_length=sequence_length,\n",
    "            input_dim=layers['input'],\n",
    "            output_dim=layers['hidden1'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden2'],\n",
    "            return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "            layers['hidden3'],\n",
    "            return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "            output_dim=layers['output'],activation='softmax'))\n",
    "    #model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop',  metrics=['accuracy'])\n",
    "    #model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def run_network(model=None, data=None):\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    \n",
    "    if data is None:\n",
    "        print ('Loading data... ')\n",
    "        # train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "        X_train, y_train  = preprocess()\n",
    "    else:\n",
    "        X_train, y_train = data\n",
    "\n",
    "    print (\"X_train, y_train,shape\")\n",
    "    print (X_train.shape)\n",
    "    print (y_train.shape)\n",
    "    print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "    if model is None:\n",
    "        model = build_model()\n",
    "        #model = build_model_2()\n",
    "        print(\"Training...\")\n",
    "        model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                validation_split=0.3)\n",
    "        model.summary()\n",
    "        print(\"Done Training...\")\n",
    "\n",
    "    #predicted = model.predict(X_test)\n",
    "    #print(\"Reshaping predicted\")\n",
    "    #predicted = np.reshape(predicted, (predicted.size,))\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"prediction exception\")\n",
    "        print 'Training duration (s) : ', time.time() - global_start_time\n",
    "        return model, y_test, 0\n",
    "   \n",
    "    try:\n",
    "        plt.figure(1)\n",
    "        plt.subplot(311)\n",
    "        plt.title(\"Actual Test Signal w/Anomalies\")\n",
    "        plt.plot(y_test[:len(y_test)], 'b')\n",
    "        plt.subplot(312)\n",
    "        plt.title(\"Predicted Signal\")\n",
    "        plt.plot(predicted[:len(y_test)], 'g')\n",
    "        plt.subplot(313)\n",
    "        plt.title(\"Squared Error\")\n",
    "        mse = ((y_test - predicted) ** 2)\n",
    "        plt.plot(mse, 'r')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"plotting exception\")\n",
    "        print (str(e))\n",
    "    print ('Training duration (s) : '% (time.time() - global_start_time))\n",
    "\n",
    "    return model, y_test, predicted\n",
    "   \"\"\"\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "# run_network()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Training...\n",
      "Train on 14208 samples, validate on 6090 samples\n",
      "Epoch 1/1\n",
      "14208/14208 [==============================] - 26s 2ms/step - loss: 2.8592 - acc: 0.2111 - val_loss: 2.9082 - val_acc: 0.1947\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 19, 64)            103936    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 19, 256)           328704    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 19, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               142800    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 341)               34441     \n",
      "=================================================================\n",
      "Total params: 609,881\n",
      "Trainable params: 609,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "if model is None:\n",
    "    model = build_model()\n",
    "    print(\"Training...\")\n",
    "    history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.3,\n",
    "            callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    model.summary()\n",
    "    print(\"Done Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "#def loadData(file): \n",
    "    # for reading also binary mode is important \n",
    "#    dbfile = open(file, 'rb')      \n",
    "#    db = pickle.load(dbfile) \n",
    "#    for keys in db: \n",
    "#        print(keys, '=>', db[keys]) \n",
    "#    dbfile.close() \n",
    "  \n",
    "#if __name__ == '__main__': \n",
    "#    loadData(\"./array_test.pickle\") \n",
    "#df_val = pd.read_pickle(\"./array_val.pickle\")\n",
    "#df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data size is that \n",
      "(21238, 19, 341)\n",
      "(21238, 341)\n",
      "X_test, y_test,shape\n",
      "(21238, 19, 341)\n",
      "(21238, 341)\n",
      "Validating...\n",
      "Done Validating...\n",
      "[[4.8210492e-05 5.9769605e-03 4.9877472e-05 ... 6.7166104e-05\n",
      "  4.7826554e-05 4.2015605e-04]\n",
      " [5.0166964e-05 6.2146150e-03 5.1796618e-05 ... 6.8828049e-05\n",
      "  4.9422135e-05 4.3281802e-04]\n",
      " [5.1167015e-05 6.7084311e-03 5.1309948e-05 ... 6.8298825e-05\n",
      "  5.1508559e-05 4.3854819e-04]\n",
      " ...\n",
      " [1.4027837e-06 1.6225490e-03 1.1321325e-06 ... 1.4115668e-06\n",
      "  1.3771767e-06 1.7109282e-05]\n",
      " [1.4946710e-06 1.6748871e-03 1.1956945e-06 ... 1.5388995e-06\n",
      "  1.4647190e-06 1.8875224e-05]\n",
      " [1.6188146e-06 1.6770544e-03 1.2679761e-06 ... 1.6032235e-06\n",
      "  1.7085524e-06 1.8586612e-05]]\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "X_test, y_test = preprocess_val()\n",
    "\n",
    "print (\"X_test, y_test,shape\")\n",
    "print (X_test.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "print(\"Validating...\")\n",
    "predicted = model.predict(X_test)\n",
    "print(\"Done Validating...\")\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did our model perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 3.00\n",
      "Validation Accuracy : 0.29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Score : %.2f'%(score))\n",
    "print('Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Loss')\n",
    "#plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [2.9082302657645718],\n",
       " 'val_acc': [0.19474548491693677],\n",
       " 'loss': [2.8591575310943096],\n",
       " 'acc': [0.21107826600331595]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Accuracy')\n",
    "#plt.plot(history.history['acc'], label='train')\n",
    "#plt.plot(history.history['val_acc'], label='test')\n",
    "#plt.legend()\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Test with new systemcall  sequence ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hidden nodes is 4000.\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "\n",
    "word_vec_length = 19\n",
    "char_vec_length = 341\n",
    "output_labels = 341\n",
    "\n",
    "\n",
    "hidden_nodes = 4000 # int(2/3 * (word_vec_length * char_vec_length))\n",
    "print(f\"The number of hidden nodes is {hidden_nodes}.\")\n",
    "\n",
    "def build_model_2():\n",
    "    # Build the model\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_nodes, return_sequences=False, input_shape=(word_vec_length, char_vec_length)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=output_labels))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    #print (\"Compilation Time : \"%(time.time() - start))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "The train data size is that \n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "X_train, y_train,shape\n",
      "(20298, 19, 341)\n",
      "(20298, 341)\n",
      "\n",
      "Data Loaded. Compiling...\n",
      "\n",
      "Build model...\n",
      "Training...\n",
      "Train on 20298 samples, validate on 21238 samples\n",
      "Epoch 1/10\n",
      "20298/20298 [==============================] - 2387s 118ms/step - loss: 3.0639 - acc: 0.2968 - val_loss: 2.7892 - val_acc: 0.4133\n",
      "Epoch 2/10\n",
      "20298/20298 [==============================] - 2296s 113ms/step - loss: 2.1557 - acc: 0.5167 - val_loss: 2.3318 - val_acc: 0.4857\n",
      "Epoch 3/10\n",
      "20298/20298 [==============================] - 2240s 110ms/step - loss: 1.5623 - acc: 0.6057 - val_loss: 2.3203 - val_acc: 0.5069\n",
      "Epoch 4/10\n",
      "20298/20298 [==============================] - 2263s 112ms/step - loss: 1.3943 - acc: 0.6450 - val_loss: 2.1959 - val_acc: 0.5048\n",
      "Epoch 5/10\n",
      "20298/20298 [==============================] - 2293s 113ms/step - loss: 1.1344 - acc: 0.6758 - val_loss: 2.1536 - val_acc: 0.5035\n",
      "Epoch 6/10\n",
      "20298/20298 [==============================] - 2309s 114ms/step - loss: 1.0304 - acc: 0.6982 - val_loss: 2.1643 - val_acc: 0.5033\n",
      "Epoch 7/10\n",
      "20298/20298 [==============================] - 2341s 115ms/step - loss: 0.9806 - acc: 0.7183 - val_loss: 2.2458 - val_acc: 0.5089\n",
      "Epoch 8/10\n",
      "20298/20298 [==============================] - 2351s 116ms/step - loss: 0.8993 - acc: 0.7389 - val_loss: 2.2913 - val_acc: 0.5078\n",
      "Epoch 9/10\n",
      "20298/20298 [==============================] - 2289s 113ms/step - loss: 0.8227 - acc: 0.7563 - val_loss: 2.2352 - val_acc: 0.5294\n",
      "Epoch 10/10\n",
      "20298/20298 [==============================] - 2275s 112ms/step - loss: 0.7431 - acc: 0.7794 - val_loss: 2.3909 - val_acc: 0.5145\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 4000)              69472000  \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 341)               1364341   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 341)               0         \n",
      "=================================================================\n",
      "Total params: 70,836,341\n",
      "Trainable params: 70,836,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Done Training...\n"
     ]
    }
   ],
   "source": [
    "global_start_time = time.time()\n",
    "    \n",
    "model=None\n",
    "\n",
    "print ('Loading data... ')\n",
    "# train on first 700 samples and test on next 300 samples (has anomaly)\n",
    "X_train, y_train  = preprocess()\n",
    "\n",
    "print (\"X_train, y_train,shape\")\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "print ('\\nData Loaded. Compiling...\\n')\n",
    "\n",
    "batch_size=32\n",
    "model = build_model_2()\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10, validation_data=(X_test, y_test))\n",
    "model.summary()\n",
    "print(\"Done Training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score : 2.39\n",
      "Validation Accuracy : 0.51\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(X_test, y_test, verbose=2, batch_size=batch_size)\n",
    "print('Score : %.2f'%(score))\n",
    "print('Validation Accuracy : %.2f'%(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## k-fold validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = X_train\n",
    "Y = y_train\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-a9b1d08b24f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mcvscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m   \u001b[1;31m# create model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    329\u001b[0m                 .format(self.n_splits, n_samples))\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BaseKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mtest_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    634\u001b[0m             raise ValueError(\n\u001b[0;32m    635\u001b[0m                 'Supported target types are: {}. Got {!r} instead.'.format(\n\u001b[1;32m--> 636\u001b[1;33m                     allowed_target_types, type_of_target_y))\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead."
     ]
    }
   ],
   "source": [
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "  # create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=341, activation='relu'))\n",
    "\tmodel.add(Dense(8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# Fit the model\n",
    "\tmodel.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "\t# evaluate the model\n",
    "\tscores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\tcvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
